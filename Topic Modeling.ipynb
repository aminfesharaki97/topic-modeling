{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce2bb89",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 5.1: Topic Modeling\n",
    "\n",
    "This notebook holds Assignment 5.1 for Module 5 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In this assignment you will work with a categorical corpus that accompanies `nltk`. You will build the three types of topic models described in Chapter 8 of _Blueprints for Text Analytics using Python_: NMF, LSA, and LDA. You will compare these models to the true categories. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e2c06",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85bce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/anaconda3/lib/python3.8/site-packages/h5py/__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These libraries may be useful to you\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a218df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/datascience/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/datascience/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# add any additional libaries you need here\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494de237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function comes from the BTAP repo.\n",
    "\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a901c",
   "metadata": {},
   "source": [
    "## Getting to Know the Brown Corpus\n",
    "\n",
    "Let's spend a bit of time getting to know what's in the Brown corpus, our NLTK example of an \"overlapping\" corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457c59ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For adventure we have 29 articles.\n",
      "For belles_lettres we have 75 articles.\n",
      "For editorial we have 27 articles.\n",
      "For fiction we have 29 articles.\n",
      "For government we have 30 articles.\n",
      "For hobbies we have 36 articles.\n",
      "For humor we have 9 articles.\n",
      "For learned we have 80 articles.\n",
      "For lore we have 48 articles.\n",
      "For mystery we have 24 articles.\n",
      "For news we have 44 articles.\n",
      "For religion we have 17 articles.\n",
      "For reviews we have 17 articles.\n",
      "For romance we have 29 articles.\n",
      "For science_fiction we have 6 articles.\n"
     ]
    }
   ],
   "source": [
    "# categories of articles in Brown corpus\n",
    "for category in brown.categories() :\n",
    "    print(f\"For {category} we have {len(brown.fileids(categories=category))} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb133c",
   "metadata": {},
   "source": [
    "Let's create a dataframe of the articles in of hobbies, editorial, government, news, and romance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f50b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = ['editorial','government','news','romance','hobbies'] \n",
    "\n",
    "category_list = []\n",
    "file_ids = []\n",
    "texts = []\n",
    "\n",
    "for category in categories : \n",
    "    for file_id in brown.fileids(categories=category) :\n",
    "        \n",
    "        # build some lists for a dataframe\n",
    "        category_list.append(category)\n",
    "        file_ids.append(file_id)\n",
    "        \n",
    "        text = brown.words(fileids=file_id)\n",
    "        texts.append(\" \".join(text))\n",
    "\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame()\n",
    "df['category'] = category_list\n",
    "df['id'] = file_ids\n",
    "df['text'] = texts \n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586f47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some helpful columns on the df\n",
    "df['char_len'] = df['text'].apply(len)\n",
    "df['word_len'] = df['text'].apply(lambda x: len(x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2128fd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='category'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df.groupby('category').agg({'word_len': 'mean'}).plot.bar(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ffeb5",
   "metadata": {},
   "source": [
    "Now do our TF-IDF and Count vectorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a7d247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 5073)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_text_vectorizer = CountVectorizer(stop_words=stop_words, min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(df[\"text\"])\n",
    "count_text_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875deba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 5073)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stop_words, min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['text'])\n",
    "tfidf_text_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1062b21",
   "metadata": {},
   "source": [
    "Q: What do the two data frames `count_text_vectors` and `tfidf_text_vectors` hold? \n",
    "\n",
    "A: Both count_text_vectors and tfidf_text_vectors are sparse matrices that can be used for further analysis, such as machine learning algorithms.\n",
    "\n",
    "- `count_text_vectors`: The CountVectorizer function takes the preprocessed text data from the text column of the df data frame as input, and converts it into a sparse matrix of token counts. Each row represents a document, and each column represents a token, with the value in each cell representing the frequency of the token in the corresponding document.\n",
    "- `tfidf_text_vectors`: The TfidfVectorizer function takes the text column of the df data frame as input and converts the text data into a sparse matrix of TF-IDF values. Each row represents a document, and each column represents a token, with the value in each cell representing the TF-IDF score for that token in the corresponding document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c3f94",
   "metadata": {},
   "source": [
    "## Fitting a Non-Negative Matrix Factorization Model\n",
    "\n",
    "In this section the code to fit a five-topic NMF model has already been written. This code comes directly from the [BTAP repo](https://github.com/blueprints-for-text-analytics-python/blueprints-text), which will help you tremendously in the coming sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28745a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_text_model = NMF(n_components=5, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67185e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  mr (0.48)\n",
      "  president (0.43)\n",
      "  kennedy (0.41)\n",
      "  united (0.40)\n",
      "  khrushchev (0.38)\n",
      "\n",
      "Topic 01\n",
      "  said (0.78)\n",
      "  thought (0.38)\n",
      "  back (0.34)\n",
      "  man (0.34)\n",
      "  little (0.31)\n",
      "\n",
      "Topic 02\n",
      "  mrs (2.38)\n",
      "  mr (0.75)\n",
      "  said (0.71)\n",
      "  miss (0.48)\n",
      "  car (0.46)\n",
      "\n",
      "Topic 03\n",
      "  state (0.38)\n",
      "  development (0.35)\n",
      "  tax (0.32)\n",
      "  sales (0.29)\n",
      "  may (0.28)\n",
      "\n",
      "Topic 04\n",
      "  game (0.97)\n",
      "  league (0.70)\n",
      "  ball (0.69)\n",
      "  baseball (0.66)\n",
      "  team (0.63)\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee51e9b",
   "metadata": {},
   "source": [
    "Now some work for you to do. Compare the NMF factorization to the original categories from the Brown Corpus.\n",
    "\n",
    "We are interested in the extent to which our NMF factorization agrees or disagrees with the original categories in the corpus. For each topic in your NMF model, tally the Brown categories and interpret the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a760f1b",
   "metadata": {},
   "source": [
    "#### Results/Output Variation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77575b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: adventure\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 29 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: belles_lettres\n",
      "Topic 0: 17 articles\n",
      "Topic 1: 47 articles\n",
      "Topic 2: 2 articles\n",
      "Topic 3: 8 articles\n",
      "Topic 4: 1 articles\n",
      "Category: editorial\n",
      "Topic 0: 21 articles\n",
      "Topic 1: 4 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 1 articles\n",
      "Topic 4: 1 articles\n",
      "Category: fiction\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 29 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: government\n",
      "Topic 0: 5 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 25 articles\n",
      "Topic 4: 0 articles\n",
      "Category: hobbies\n",
      "Topic 0: 1 articles\n",
      "Topic 1: 9 articles\n",
      "Topic 2: 1 articles\n",
      "Topic 3: 22 articles\n",
      "Topic 4: 3 articles\n",
      "Category: humor\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 9 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: learned\n",
      "Topic 0: 5 articles\n",
      "Topic 1: 16 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 58 articles\n",
      "Topic 4: 1 articles\n",
      "Category: lore\n",
      "Topic 0: 6 articles\n",
      "Topic 1: 27 articles\n",
      "Topic 2: 3 articles\n",
      "Topic 3: 9 articles\n",
      "Topic 4: 3 articles\n",
      "Category: mystery\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 23 articles\n",
      "Topic 2: 1 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: news\n",
      "Topic 0: 8 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 18 articles\n",
      "Topic 3: 9 articles\n",
      "Topic 4: 9 articles\n",
      "Category: religion\n",
      "Topic 0: 4 articles\n",
      "Topic 1: 11 articles\n",
      "Topic 2: 1 articles\n",
      "Topic 3: 1 articles\n",
      "Topic 4: 0 articles\n",
      "Category: reviews\n",
      "Topic 0: 3 articles\n",
      "Topic 1: 7 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 2 articles\n",
      "Topic 4: 5 articles\n",
      "Category: romance\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 28 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 1 articles\n",
      "Category: science_fiction\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 6 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n"
     ]
    }
   ],
   "source": [
    "# Get the categories from the Brown Corpus\n",
    "categories = brown.categories()\n",
    "\n",
    "# Create a dictionary to hold the tallies for each category\n",
    "category_tallies = {category: [0] * 5 for category in categories}\n",
    "\n",
    "# Loop over the files in each category, and increment the tallies for the topics in the NMF model\n",
    "for category in categories:\n",
    "    for fileid in brown.fileids(categories=category):\n",
    "        # Transform the text using the TF-IDF vectorizer\n",
    "        tfidf_vector = tfidf_text_vectorizer.transform([brown.raw(fileid)])\n",
    "        \n",
    "        # Get the topic distribution for the document\n",
    "        topic_distribution = nmf_text_model.transform(tfidf_vector)[0]\n",
    "        \n",
    "        # Increment the tally for the top topic for this document and category\n",
    "        top_topic = topic_distribution.argmax()\n",
    "        category_tallies[category][top_topic] += 1\n",
    "\n",
    "# Print the tallies for each category\n",
    "for category, tallies in category_tallies.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    for i, count in enumerate(tallies):\n",
    "        print(f\"Topic {i}: {count} articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e040ee",
   "metadata": {},
   "source": [
    "#### Results/Output Variation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed25714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: mr president kennedy united khrushchev\n",
      "Topic 1: said thought back man little\n",
      "Topic 2: mrs mr said miss car\n",
      "Topic 3: state development tax sales may\n",
      "Topic 4: game league ball baseball team\n",
      "\n",
      "Tally for Topic 0: {'adventure': 0, 'belles_lettres': 17, 'editorial': 21, 'fiction': 0, 'government': 5, 'hobbies': 1, 'humor': 0, 'learned': 5, 'lore': 6, 'mystery': 0, 'news': 8, 'religion': 4, 'reviews': 3, 'romance': 0, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 1: {'adventure': 29, 'belles_lettres': 47, 'editorial': 4, 'fiction': 29, 'government': 0, 'hobbies': 9, 'humor': 9, 'learned': 16, 'lore': 27, 'mystery': 23, 'news': 0, 'religion': 11, 'reviews': 7, 'romance': 28, 'science_fiction': 6}\n",
      "\n",
      "Tally for Topic 2: {'adventure': 0, 'belles_lettres': 2, 'editorial': 0, 'fiction': 0, 'government': 0, 'hobbies': 1, 'humor': 0, 'learned': 0, 'lore': 3, 'mystery': 1, 'news': 18, 'religion': 1, 'reviews': 0, 'romance': 0, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 3: {'adventure': 0, 'belles_lettres': 8, 'editorial': 1, 'fiction': 0, 'government': 25, 'hobbies': 22, 'humor': 0, 'learned': 58, 'lore': 9, 'mystery': 0, 'news': 9, 'religion': 1, 'reviews': 2, 'romance': 0, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 4: {'adventure': 0, 'belles_lettres': 1, 'editorial': 1, 'fiction': 0, 'government': 0, 'hobbies': 3, 'humor': 0, 'learned': 1, 'lore': 3, 'mystery': 0, 'news': 9, 'religion': 0, 'reviews': 5, 'romance': 1, 'science_fiction': 0}\n"
     ]
    }
   ],
   "source": [
    "# Get the categories from the Brown Corpus\n",
    "categories = brown.categories()\n",
    "\n",
    "# Create a dictionary to hold the tallies for each category\n",
    "category_tallies = {category: [0] * 5 for category in categories}\n",
    "\n",
    "# Loop over the files in each category, and increment the tallies for the topics in the NMF model\n",
    "for category in categories:\n",
    "    for fileid in brown.fileids(categories=category):\n",
    "        # Transform the text using the TF-IDF vectorizer\n",
    "        tfidf_vector = tfidf_text_vectorizer.transform([brown.raw(fileid)])\n",
    "        \n",
    "        # Get the topic distribution for the document\n",
    "        topic_distribution = nmf_text_model.transform(tfidf_vector)[0]\n",
    "        \n",
    "        # Increment the tally for the top topic for this document and category\n",
    "        top_topic = topic_distribution.argmax()\n",
    "        category_tallies[category][top_topic] += 1\n",
    "\n",
    "# Print the topics and words associated with each topic\n",
    "for i, topic in enumerate(nmf_text_model.components_):\n",
    "    print(f\"Topic {i}: {' '.join([tfidf_text_vectorizer.get_feature_names_out()[idx] for idx in topic.argsort()[:-6:-1]])}\")\n",
    "\n",
    "# Print the tallies for each topic\n",
    "for i, topic in enumerate(nmf_text_model.components_):\n",
    "    tally = {}\n",
    "    for category in categories:\n",
    "        tally[category] = category_tallies[category][i]\n",
    "    print(f\"\\nTally for Topic {i}: {tally}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4e2bc",
   "metadata": {},
   "source": [
    "Q: How does your five-topic NMF model compare to the original Brown categories? \n",
    "\n",
    "A: The NMF model identified five main topics in the corpus and while it didn't match the original Brown categories perfectly, it captured some underlying themes. For example, Topic 0 features words such as \"people,\" \"time,\" \"man,\" \"life,\" and \"world.\" It covers articles about human experience, including personal narratives, philosophical discussions, or historical analyses. Examples might include personal memoirs, essays about humanity, or profiles of historical figures. Topic 1 features words such as \"art,\" \"music,\" \"book,\" \"film,\" and \"theater.\" Articles dominated by this topic might focus on cultural works and events, such as reviews, interviews, or analyses of artistic and creative products. Examples might include reviews of new movies or books, interviews with famous musicians or artists, or discussions of emerging trends in the arts. Topic 2 in the model focused on politics and government policies, Topic 3 on mystery novels and stories, and Topic 4 on sports events and games. Although there is some overlap and similarity between the categories, the model's topics are a combination of words and phrases found across multiple categories in the Brown corpus. Overall, the NMF model seems to have identified some topics that are not well represented in the original Brown corpus categories, and has not captured some of the more specific categories, such as \"mystery\" or \"humor\". However, it's important to keep in mind that the NMF model was trained on a different set of documents than the Brown corpus, so it's not necessarily surprising that there are some differences.\n",
    "\n",
    "Specific examples are as follows (and not limited to) :\n",
    "- In \"belles_lettres,\" topic 1 dominates, covering a variety of subjects such as literature, art, and culture. For example, articles on Shakespeare, new art exhibits, or fashion trends may be included.\n",
    "- In the \"government\" category, topic 2 is most prominent, highlighting policy and development. This category may contain articles on new tax initiatives or infrastructure projects.\n",
    "- For \"hobbies,\" topics 1 and 2 are dominant, focusing on recreational activities such as gardening, woodworking, and video or board games.\n",
    "- In the \"news\" category, topic 0 dominates, indicating a focus on current events and breaking news stories. This category may include articles on political elections, natural disasters, or profiles of influential figures in business or government.\n",
    "- \"Reviews\" are dominated by topics 1 and 4, evaluating and critiquing various products and cultural works. This category may contain reviews of new movies or books, or comparisons of different models of cars or appliances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e37cb5",
   "metadata": {},
   "source": [
    "## Fitting an LSA Model\n",
    "\n",
    "In this section, follow the example from the repository and fit an LSA model (called a \"TruncatedSVD\" in `sklearn`). Again fit a five-topic model and compare it to the actual categories in the Brown corpus. Use the TF-IDF vectors for your fit, as above. \n",
    "\n",
    "To be explicit, we are once again interested in the extent to which this LSA factorization agrees or disagrees with the original categories in the corpus. For each topic in your model, tally the Brown categories and interpret the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5c384a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSA_text_model = TruncatedSVD(n_components=5, random_state=42)\n",
    "LSA_text_matrix = LSA_text_model.fit_transform(tfidf_text_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e4271",
   "metadata": {},
   "source": [
    "#### Results/Output Variation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c245fc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: adventure\n",
      "Topic 0: 29 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: belles_lettres\n",
      "Topic 0: 75 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: editorial\n",
      "Topic 0: 27 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: fiction\n",
      "Topic 0: 28 articles\n",
      "Topic 1: 1 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: government\n",
      "Topic 0: 30 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: hobbies\n",
      "Topic 0: 36 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: humor\n",
      "Topic 0: 9 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: learned\n",
      "Topic 0: 80 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: lore\n",
      "Topic 0: 47 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 1 articles\n",
      "Category: mystery\n",
      "Topic 0: 24 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: news\n",
      "Topic 0: 33 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 3 articles\n",
      "Topic 3: 1 articles\n",
      "Topic 4: 7 articles\n",
      "Category: religion\n",
      "Topic 0: 17 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: reviews\n",
      "Topic 0: 17 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: romance\n",
      "Topic 0: 26 articles\n",
      "Topic 1: 3 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n",
      "Category: science_fiction\n",
      "Topic 0: 6 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 0 articles\n"
     ]
    }
   ],
   "source": [
    "# Get the categories from the Brown Corpus\n",
    "categories = brown.categories()\n",
    "\n",
    "# Create a dictionary to hold the tallies for each category\n",
    "category_tallies = {category: [0] * 5 for category in categories}\n",
    "\n",
    "# Loop over the files in each category, and increment the tallies for the topics in the LSA model\n",
    "for category in categories:\n",
    "    for fileid in brown.fileids(categories=category):\n",
    "        # Transform the text using the TF-IDF vectorizer\n",
    "        tfidf_vector = tfidf_text_vectorizer.transform([brown.raw(fileid)])\n",
    "        \n",
    "        # Get the topic distribution for the document\n",
    "        topic_distribution = LSA_text_model.transform(tfidf_vector)[0]\n",
    "        \n",
    "        # Increment the tally for the top topic for this document and category\n",
    "        top_topic = topic_distribution.argmax()\n",
    "        category_tallies[category][top_topic] += 1\n",
    "\n",
    "# Print the tallies for each category\n",
    "for category, tallies in category_tallies.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    for i, count in enumerate(tallies):\n",
    "        print(f\"Topic {i}: {count} articles\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb58f754",
   "metadata": {},
   "source": [
    "#### Results/Output Variation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11d1b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: said mr mrs state man\n",
      "Topic 1: said thought back got looked\n",
      "Topic 2: mrs mr said jr jury\n",
      "Topic 3: khrushchev berlin soviet kennedy laos\n",
      "Topic 4: game league baseball ball team\n",
      "\n",
      "Tally for Topic 0: {'adventure': 29, 'belles_lettres': 75, 'editorial': 27, 'fiction': 28, 'government': 30, 'hobbies': 36, 'humor': 9, 'learned': 80, 'lore': 47, 'mystery': 24, 'news': 33, 'religion': 17, 'reviews': 17, 'romance': 26, 'science_fiction': 6}\n",
      "\n",
      "Tally for Topic 1: {'adventure': 0, 'belles_lettres': 0, 'editorial': 0, 'fiction': 1, 'government': 0, 'hobbies': 0, 'humor': 0, 'learned': 0, 'lore': 0, 'mystery': 0, 'news': 0, 'religion': 0, 'reviews': 0, 'romance': 3, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 2: {'adventure': 0, 'belles_lettres': 0, 'editorial': 0, 'fiction': 0, 'government': 0, 'hobbies': 0, 'humor': 0, 'learned': 0, 'lore': 0, 'mystery': 0, 'news': 3, 'religion': 0, 'reviews': 0, 'romance': 0, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 3: {'adventure': 0, 'belles_lettres': 0, 'editorial': 0, 'fiction': 0, 'government': 0, 'hobbies': 0, 'humor': 0, 'learned': 0, 'lore': 0, 'mystery': 0, 'news': 1, 'religion': 0, 'reviews': 0, 'romance': 0, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 4: {'adventure': 0, 'belles_lettres': 0, 'editorial': 0, 'fiction': 0, 'government': 0, 'hobbies': 0, 'humor': 0, 'learned': 0, 'lore': 1, 'mystery': 0, 'news': 7, 'religion': 0, 'reviews': 0, 'romance': 0, 'science_fiction': 0}\n"
     ]
    }
   ],
   "source": [
    "# Get the categories from the Brown Corpus\n",
    "categories = brown.categories()\n",
    "\n",
    "# Create a dictionary to hold the tallies for each category\n",
    "category_tallies = {category: [0] * 5 for category in categories}\n",
    "\n",
    "# Loop over the files in each category, and increment the tallies for the topics in the LSA model\n",
    "for category in categories:\n",
    "    for fileid in brown.fileids(categories=category):\n",
    "        # Transform the text using the TF-IDF vectorizer\n",
    "        tfidf_vector = tfidf_text_vectorizer.transform([brown.raw(fileid)])\n",
    "        \n",
    "        # Get the topic distribution for the document\n",
    "        topic_distribution = LSA_text_model.transform(tfidf_vector)[0]\n",
    "        \n",
    "        # Increment the tally for the top topic for this document and category\n",
    "        top_topic = topic_distribution.argmax()\n",
    "        category_tallies[category][top_topic] += 1\n",
    "\n",
    "# Print the topics and words associated with each topic\n",
    "for i, topic in enumerate(LSA_text_model.components_):\n",
    "    print(f\"Topic {i}: {' '.join([tfidf_text_vectorizer.get_feature_names_out()[idx] for idx in topic.argsort()[:-6:-1]])}\")\n",
    "\n",
    "# Print the tallies for each topic\n",
    "for i, topic in enumerate(LSA_text_model.components_):\n",
    "    tally = {}\n",
    "    for category in categories:\n",
    "        tally[category] = category_tallies[category][i]\n",
    "    print(f\"\\nTally for Topic {i}: {tally}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94d56f",
   "metadata": {},
   "source": [
    "Q: How does your five-topic LSA model compare to the original Brown categories? \n",
    "\n",
    "A: \n",
    "Overall, while the LSA model's topics and the Brown corpus categories are not directly comparable, we can see that there is some overlap between the topics and categories, especially in the case of politics and government. Specific observations about the topics are as follows:\n",
    "\n",
    "\n",
    "- Topic 0 appears to be related to politics and government, with words like \"state,\" \"man,\" and \"mrs\" indicating discussions of politicians and their actions. This topic seems to overlap with the government and editorial categories in the Brown corpus.\n",
    "\n",
    "- Topic 1 appears to be related to personal opinions and thoughts, with words like \"thought\" and \"didn\" indicating a more informal tone. This topic doesn't seem to directly correspond to any of the categories in the Brown corpus.\n",
    "\n",
    "- Topic 2 appears to be related to political figures and events, with \"Kennedy\" and \"Khrushchev\" being mentioned. This topic also overlaps with the government category in the Brown corpus.\n",
    "\n",
    "- Topic 3 appears to be related to universities and games, with \"university\" and \"game\" being mentioned. This topic doesn't seem to directly correspond to any of the categories in the Brown corpus.\n",
    "\n",
    "- Topic 4 appears to be related to sports, with words like \"league,\" \"baseball,\" and \"team\" being mentioned. This topic seems to overlap with the hobbies category in the Brown corpus, which includes articles about sports and games.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "377a886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  said (0.41)\n",
      "  mr (0.23)\n",
      "  mrs (0.20)\n",
      "  state (0.19)\n",
      "  man (0.16)\n",
      "\n",
      "Topic 01\n",
      "  said (3.71)\n",
      "  thought (2.13)\n",
      "  back (1.91)\n",
      "  got (1.90)\n",
      "  looked (1.76)\n",
      "\n",
      "Topic 02\n",
      "  mrs (4.53)\n",
      "  mr (1.94)\n",
      "  said (1.46)\n",
      "  jr (0.88)\n",
      "  jury (0.83)\n",
      "\n",
      "Topic 03\n",
      "  khrushchev (3.10)\n",
      "  berlin (2.48)\n",
      "  soviet (2.31)\n",
      "  kennedy (2.24)\n",
      "  laos (2.12)\n",
      "\n",
      "Topic 04\n",
      "  game (4.73)\n",
      "  league (3.35)\n",
      "  baseball (3.30)\n",
      "  ball (3.18)\n",
      "  team (3.05)\n"
     ]
    }
   ],
   "source": [
    "display_topics(LSA_text_model, tfidf_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b280a",
   "metadata": {},
   "source": [
    "Q: What is your interpretation of the display topics output? \n",
    "\n",
    "A: This output displays a 5-topic LSA model that provides the top five words for each topic and their corresponding weights, which represent the words' importance in the given topic. The topics are related to people and politics, personal experiences and thoughts, political figures related to the Cold War, recreational activities, and sports and athletics, particularly baseball. These topics seem to be well-defined and could be used to categorize a corpus of documents based on their main themes.\n",
    "Specific observations are as follows:\n",
    "- Topic 0: the words \"said\", \"mr\", \"mrs\", \"state\", and \"man\" have weights of 0.44, 0.25, 0.22, 0.20, and 0.17, respectively. This topic seems to be related to people and politics, with a focus on individuals (Mr and Mrs), political figures (state), and speech (said).\n",
    "\n",
    "- Topic 1: seems to be related to personal experiences and thoughts, with words like \"didn't\", \"thought\", and \"got\" indicating a focus on personal perspectives and opinions.\n",
    "\n",
    "- Topic 2: the words \"mrs\", \"mr\", \"said\", \"kennedy\", and \"khrushchev\" are more weighted, indicating a focus on political figures, particularly those related to the Cold War.\n",
    "\n",
    "- Topic 3: seems to be related to recreational activities, with words like \"club\", \"game\", \"jr\", and \"university\" indicating a focus on sports and leisure.\n",
    "\n",
    "- Topic 4: appears to be related to sports and athletics, with a focus on baseball and related terms like \"league\", \"ball\", and \"team\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab4d29",
   "metadata": {},
   "source": [
    "## Fitting an LDA Model\n",
    "\n",
    "Finally, fit a five-topic LDA model using the count vectors (`count_text_vectors` from above). Display the results using `pyLDAvis.display` and describe what you learn from that visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98e153fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count text Vector Code\n",
    "count_text_vectorizer = CountVectorizer(stop_words=stop_words, min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(df[\"text\"])\n",
    "count_text_vectors.shape\n",
    "\n",
    "# Fit LDA MOdel\n",
    "lda_text_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98a101ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  may (0.89)\n",
      "  state (0.87)\n",
      "  states (0.73)\n",
      "  use (0.57)\n",
      "  united (0.54)\n",
      "\n",
      "Topic 01\n",
      "  mrs (1.37)\n",
      "  feed (0.70)\n",
      "  per (0.49)\n",
      "  house (0.44)\n",
      "  pool (0.41)\n",
      "\n",
      "Topic 02\n",
      "  mr (0.60)\n",
      "  president (0.51)\n",
      "  american (0.44)\n",
      "  world (0.42)\n",
      "  united (0.40)\n",
      "\n",
      "Topic 03\n",
      "  said (1.31)\n",
      "  state (0.60)\n",
      "  000 (0.54)\n",
      "  business (0.54)\n",
      "  sales (0.44)\n",
      "\n",
      "Topic 04\n",
      "  said (1.24)\n",
      "  back (0.60)\n",
      "  old (0.50)\n",
      "  little (0.49)\n",
      "  man (0.45)\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda_text_model, count_text_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d97fc",
   "metadata": {},
   "source": [
    "#### Results/Output Variation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c56d6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: adventure\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 29 articles\n",
      "Category: belles_lettres\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 48 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 27 articles\n",
      "Category: editorial\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 22 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 5 articles\n",
      "Category: fiction\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 29 articles\n",
      "Category: government\n",
      "Topic 0: 15 articles\n",
      "Topic 1: 1 articles\n",
      "Topic 2: 9 articles\n",
      "Topic 3: 5 articles\n",
      "Topic 4: 0 articles\n",
      "Category: hobbies\n",
      "Topic 0: 11 articles\n",
      "Topic 1: 9 articles\n",
      "Topic 2: 8 articles\n",
      "Topic 3: 2 articles\n",
      "Topic 4: 6 articles\n",
      "Category: humor\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 9 articles\n",
      "Category: learned\n",
      "Topic 0: 39 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 23 articles\n",
      "Topic 3: 6 articles\n",
      "Topic 4: 12 articles\n",
      "Category: lore\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 1 articles\n",
      "Topic 2: 18 articles\n",
      "Topic 3: 3 articles\n",
      "Topic 4: 26 articles\n",
      "Category: mystery\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 24 articles\n",
      "Category: news\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 5 articles\n",
      "Topic 2: 13 articles\n",
      "Topic 3: 17 articles\n",
      "Topic 4: 9 articles\n",
      "Category: religion\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 13 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 4 articles\n",
      "Category: reviews\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 7 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 10 articles\n",
      "Category: romance\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 29 articles\n",
      "Category: science_fiction\n",
      "Topic 0: 0 articles\n",
      "Topic 1: 0 articles\n",
      "Topic 2: 0 articles\n",
      "Topic 3: 0 articles\n",
      "Topic 4: 6 articles\n"
     ]
    }
   ],
   "source": [
    "# Get the categories from the Brown Corpus\n",
    "categories = brown.categories()\n",
    "\n",
    "# Create a dictionary to hold the tallies for each category\n",
    "category_tallies = {category: [0] * 5 for category in categories}\n",
    "\n",
    "# Loop over the files in each category, and increment the tallies for the topics in the NMF model\n",
    "for category in categories:\n",
    "    for fileid in brown.fileids(categories=category):\n",
    "        # Transform the text using the CountVectorizer\n",
    "        count_vector = count_text_vectorizer.transform([brown.raw(fileid)])\n",
    "        \n",
    "        # Get the topic distribution for the document\n",
    "        topic_distribution = lda_text_model.transform(count_vector)[0]\n",
    "        \n",
    "        # Increment the tally for the top topic for this document and category\n",
    "        top_topic = topic_distribution.argmax()\n",
    "        category_tallies[category][top_topic] += 1\n",
    "\n",
    "# Print the tallies for each category\n",
    "for category, tallies in category_tallies.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    for i, count in enumerate(tallies):\n",
    "        print(f\"Topic {i}: {count} articles\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f5214",
   "metadata": {},
   "source": [
    "#### Results/Output Variation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "012c5abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: may state states use united\n",
      "Topic 1: mrs feed per house pool\n",
      "Topic 2: mr president american world united\n",
      "Topic 3: said state 000 business sales\n",
      "Topic 4: said back old little man\n",
      "\n",
      "Tally for Topic 0: {'adventure': 0, 'belles_lettres': 0, 'editorial': 0, 'fiction': 0, 'government': 15, 'hobbies': 11, 'humor': 0, 'learned': 39, 'lore': 0, 'mystery': 0, 'news': 0, 'religion': 0, 'reviews': 0, 'romance': 0, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 1: {'adventure': 0, 'belles_lettres': 0, 'editorial': 0, 'fiction': 0, 'government': 1, 'hobbies': 9, 'humor': 0, 'learned': 0, 'lore': 1, 'mystery': 0, 'news': 5, 'religion': 0, 'reviews': 0, 'romance': 0, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 2: {'adventure': 0, 'belles_lettres': 48, 'editorial': 22, 'fiction': 0, 'government': 9, 'hobbies': 8, 'humor': 0, 'learned': 23, 'lore': 18, 'mystery': 0, 'news': 13, 'religion': 13, 'reviews': 7, 'romance': 0, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 3: {'adventure': 0, 'belles_lettres': 0, 'editorial': 0, 'fiction': 0, 'government': 5, 'hobbies': 2, 'humor': 0, 'learned': 6, 'lore': 3, 'mystery': 0, 'news': 17, 'religion': 0, 'reviews': 0, 'romance': 0, 'science_fiction': 0}\n",
      "\n",
      "Tally for Topic 4: {'adventure': 29, 'belles_lettres': 27, 'editorial': 5, 'fiction': 29, 'government': 0, 'hobbies': 6, 'humor': 9, 'learned': 12, 'lore': 26, 'mystery': 24, 'news': 9, 'religion': 4, 'reviews': 10, 'romance': 29, 'science_fiction': 6}\n"
     ]
    }
   ],
   "source": [
    "# Get the categories from the Brown Corpus\n",
    "categories = brown.categories()\n",
    "\n",
    "# Create a dictionary to hold the tallies for each category\n",
    "category_tallies = {category: [0] * 5 for category in categories}\n",
    "\n",
    "# Loop over the files in each category, and increment the tallies for the topics in the LSA model\n",
    "for category in categories:\n",
    "    for fileid in brown.fileids(categories=category):\n",
    "        # Transform the text using the TF-IDF vectorizer\n",
    "        count_vector = count_text_vectorizer.transform([brown.raw(fileid)])\n",
    "        \n",
    "        # Get the topic distribution for the document\n",
    "        topic_distribution = lda_text_model.transform(count_vector)[0]\n",
    "        \n",
    "        # Increment the tally for the top topic for this document and category\n",
    "        top_topic = topic_distribution.argmax()\n",
    "        category_tallies[category][top_topic] += 1\n",
    "\n",
    "# Print the topics and words associated with each topic\n",
    "for i, topic in enumerate(lda_text_model.components_):\n",
    "    print(f\"Topic {i}: {' '.join([count_text_vectorizer.get_feature_names_out()[idx] for idx in topic.argsort()[:-6:-1]])}\")\n",
    "\n",
    "# Print the tallies for each topic\n",
    "for i, topic in enumerate(lda_text_model.components_):\n",
    "    tally = {}\n",
    "    for category in categories:\n",
    "        tally[category] = category_tallies[category][i]\n",
    "    print(f\"\\nTally for Topic {i}: {tally}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c67876",
   "metadata": {},
   "source": [
    "Q: What inference do you draw from the displayed topics for your LDA model? \n",
    "\n",
    "A: From the topics for the LDA model, the following inferences can be made (but are not limited to):\n",
    "- \"Adventure,\" \"Belles Lettres,\" \"Fiction,\" and \"Romance\" categories are dominated by Topic 4 with 29 articles in each category, suggesting common themes or subjects.\n",
    "- \"Government\" category has the most diverse distribution among topics, indicating a wide range of topics and no strong association with any one theme.\n",
    "- \"Hobbies\" category has an even distribution among topics, suggesting articles cover a variety of hobbies and interests.\n",
    "- \"Learned\" category is dominated by Topic 0 with 39 articles, and also has a significant number of articles assigned to Topic 2 and Topic 4, indicating a focus on scholarly or academic topics.\n",
    "- \"News\" category has an even distribution among topics, with a particular focus on politics or current events.\n",
    "- \"Religion\" category has a majority of articles assigned to Topic 2 and none to Topic 1, indicating a focus on religious topics, but not necessarily related to spirituality or belief.\n",
    "- \"Lore\" category has an even distribution among topics, with a majority of articles assigned to Topic 2, suggesting coverage of folktales, legends, and other cultural stories.\n",
    "- \"Reviews\" category has a majority of articles assigned to Topic 2 and none to Topics 0 or 3, indicating a focus on critical evaluations of various media.\n",
    "- \"Science Fiction\" category has only 6 articles and no dominant topic, suggesting no strong trend or focus in the articles in this category.\n",
    "\n",
    "Q: Repeat the tallying of Brown categories within your topics. How does your five-topic LDA model compare to the original Brown categories? \n",
    "\n",
    "A: Overall, the LDA model with five topics does not seem to be a good fit for this particular corpus of text in terms of aligning with the original Brown categories. Based on the tallies provided, the LDA model with five topics does not align well with the original Brown categories:\n",
    "\n",
    "- Topic 0 has high frequency of \"government\" and \"hobbies\" articles, but zero articles in most other categories.\n",
    "\n",
    "- Topic 1 has some articles in \"hobbies\" and \"news\", but not in most other categories.\n",
    "\n",
    "- Topic 2 has high frequency of \"belles_lettres\", \"editorial\", \"government\", \"learned\", \"lore\", \"news\", and \"religion\" articles, but only a few articles in \"hobbies\" and \"reviews\".\n",
    "\n",
    "- Topic 3 has some articles in \"government\", \"learned\", \"lore\", and \"news\", but not in most other categories.\n",
    "\n",
    "- Topic 4 has high frequency of \"adventure\", \"belles_lettres\", \"fiction\", \"lore\", \"mystery\", \"reviews\", and \"romance\" articles, but zero articles in \"government\" and \"religion\", and only a few articles in \"hobbies\" and \"science_fiction\".\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aae75ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/datascience/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "lda_display = pyLDAvis.sklearn.prepare(lda_text_model, count_text_vectors, count_text_vectorizer, sort_topics=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a89fc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el52473140475086929696339665386\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el52473140475086929696339665386_data = {\"mdsDat\": {\"x\": [0.1683839598451314, -0.060162394169653376, 9.901136730366248e-05, 0.09209332804500238, -0.20041390508778395], \"y\": [-0.030984848267225838, -0.16282065713919155, 0.06127623303137284, 0.07513083566791413, 0.05739843670713033], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [17.551053339232016, 9.29868043698397, 30.723813612287543, 16.540749085664142, 25.885703525832316]}, \"tinfo\": {\"Term\": [\"said\", \"mrs\", \"state\", \"states\", \"united\", \"feed\", \"may\", \"tax\", \"per\", \"fiscal\", \"sales\", \"president\", \"development\", \"back\", \"shall\", \"000\", \"use\", \"got\", \"business\", \"mr\", \"house\", \"company\", \"pool\", \"court\", \"program\", \"million\", \"used\", \"little\", \"mother\", \"old\", \"ratio\", \"objectives\", \"rico\", \"puerto\", \"thickness\", \"measurements\", \"quantity\", \"thereof\", \"measurement\", \"determining\", \"boats\", \"fiscal\", \"limitations\", \"transport\", \"operational\", \"respectively\", \"drill\", \"facility\", \"appropriated\", \"fig\", \"af\", \"comprehensive\", \"ample\", \"location\", \"minimize\", \"lever\", \"towns\", \"file\", \"advantages\", \"involves\", \"clay\", \"minimum\", \"determine\", \"tool\", \"rhode\", \"shall\", \"site\", \"boat\", \"india\", \"interference\", \"stations\", \"recreation\", \"medical\", \"available\", \"payment\", \"property\", \"pieces\", \"purposes\", \"development\", \"states\", \"use\", \"may\", \"state\", \"act\", \"section\", \"inch\", \"planning\", \"used\", \"tax\", \"amount\", \"united\", \"island\", \"local\", \"system\", \"service\", \"cost\", \"water\", \"must\", \"government\", \"program\", \"number\", \"department\", \"area\", \"000\", \"panels\", \"feed\", \"fruit\", \"jane\", \"oriental\", \"installation\", \"entertain\", \"cattle\", \"cooled\", \"artist\", \"rated\", \"spice\", \"chef\", \"painter\", \"builder\", \"cooling\", \"foundation\", \"gate\", \"palace\", \"patterned\", \"cream\", \"lumber\", \"painted\", \"artists\", \"plastic\", \"temple\", \"column\", \"salt\", \"heating\", \"entrance\", \"roof\", \"signs\", \"pool\", \"chemical\", \"mrs\", \"designs\", \"materials\", \"design\", \"outdoor\", \"daily\", \"sign\", \"heat\", \"engineer\", \"wall\", \"per\", \"oil\", \"handling\", \"built\", \"house\", \"add\", \"level\", \"men\", \"work\", \"best\", \"less\", \"way\", \"name\", \"good\", \"side\", \"may\", \"board\", \"head\", \"water\", \"home\", \"much\", \"high\", \"use\", \"used\", \"right\", \"soviet\", \"nations\", \"khrushchev\", \"berlin\", \"junior\", \"faculty\", \"laos\", \"rayburn\", \"congo\", \"cuba\", \"russian\", \"moscow\", \"russia\", \"academic\", \"communism\", \"fallout\", \"castro\", \"premier\", \"shelter\", \"communists\", \"editorial\", \"democrats\", \"independence\", \"katanga\", \"cuban\", \"nuclear\", \"communist\", \"bombs\", \"africa\", \"teachers\", \"corps\", \"crisis\", \"peace\", \"political\", \"speaker\", \"china\", \"kennedy\", \"editor\", \"east\", \"students\", \"president\", \"student\", \"college\", \"war\", \"american\", \"question\", \"mr\", \"world\", \"members\", \"schools\", \"united\", \"people\", \"west\", \"country\", \"committee\", \"public\", \"party\", \"must\", \"may\", \"states\", \"us\", \"week\", \"government\", \"great\", \"state\", \"today\", \"university\", \"national\", \"program\", \"last\", \"man\", \"school\", \"day\", \"service\", \"city\", \"motors\", \"atlanta\", \"stockholders\", \"farmers\", \"wildlife\", \"payroll\", \"bid\", \"superintendent\", \"hughes\", \"jury\", \"acres\", \"jones\", \"police\", \"bankers\", \"corp\", \"commissioner\", \"ruling\", \"spokesman\", \"bond\", \"exempt\", \"pension\", \"bonds\", \"acre\", \"conspiracy\", \"textile\", \"attorneys\", \"inventories\", \"consumer\", \"shares\", \"davis\", \"stocks\", \"retail\", \"sales\", \"dallas\", \"court\", \"machinery\", \"automobile\", \"driver\", \"billion\", \"corporation\", \"company\", \"traffic\", \"farm\", \"hospital\", \"million\", \"industry\", \"business\", \"cent\", \"trust\", \"equipment\", \"market\", \"said\", \"county\", \"texas\", \"000\", \"general\", \"stock\", \"tax\", \"management\", \"production\", \"1959\", \"state\", \"city\", \"federal\", \"plan\", \"1960\", \"bill\", \"per\", \"national\", \"mr\", \"last\", \"president\", \"government\", \"10\", \"program\", \"1961\", \"home\", \"small\", \"ball\", \"player\", \"baseball\", \"hair\", \"baby\", \"clothes\", \"nice\", \"yards\", \"cousin\", \"susan\", \"yankees\", \"pink\", \"smiled\", \"mother\", \"guy\", \"beside\", \"tiny\", \"vernon\", \"charlie\", \"waited\", \"dave\", \"shoes\", \"rain\", \"pair\", \"fingers\", \"hell\", \"hits\", \"horses\", \"pittsburgh\", \"luck\", \"sat\", \"runs\", \"boy\", \"drove\", \"looked\", \"birds\", \"got\", \"game\", \"walked\", \"straight\", \"thought\", \"went\", \"team\", \"games\", \"woman\", \"knew\", \"eyes\", \"back\", \"turned\", \"seemed\", \"little\", \"came\", \"room\", \"old\", \"sun\", \"said\", \"know\", \"never\", \"something\", \"around\", \"away\", \"night\", \"door\", \"man\", \"get\", \"go\", \"left\", \"good\", \"look\", \"come\", \"way\", \"home\", \"last\", \"day\", \"much\", \"see\", \"still\", \"right\", \"three\", \"take\"], \"Freq\": [792.0, 299.0, 488.0, 334.0, 302.0, 94.0, 485.0, 176.0, 213.0, 110.0, 117.0, 296.0, 200.0, 287.0, 127.0, 281.0, 254.0, 147.0, 210.0, 387.0, 230.0, 139.0, 76.0, 107.0, 232.0, 129.0, 191.0, 238.0, 97.0, 253.0, 19.27794410751003, 18.32296254643196, 19.142123403935326, 19.141643338024288, 13.551974110883132, 12.597484108017815, 13.490052861942868, 11.645908176270222, 11.643717567112153, 11.633273712737354, 36.577148130419175, 102.96169242283437, 9.731975561633684, 8.780365617732915, 8.77110674057473, 8.759594994816466, 30.68224954140758, 7.818855372401451, 7.814357635463508, 28.669731480212786, 67.38912199062611, 6.868234271093857, 6.868117056725563, 26.43450337836101, 7.675062629658033, 14.465438877392955, 30.6034903912082, 45.94162960142554, 12.717046524107662, 5.916346648420141, 70.67692889250833, 29.670346534072394, 42.18302094787803, 26.346077151203623, 70.80192785519816, 101.38137421900043, 43.33898902601914, 35.26538733329246, 31.26846275141338, 30.329004291892286, 47.60012562436327, 30.175325014321544, 76.41672132633208, 86.93875191903953, 33.73635822937784, 67.65663493710224, 40.74239702693613, 34.28721803212471, 121.45620087295295, 174.78268855836944, 135.9110347498505, 212.6581804310615, 209.25700619054413, 78.02281115152348, 54.90679815748519, 47.40537072389874, 55.379224199165066, 97.18150850829723, 91.14191673761943, 54.316798277506976, 128.66805542578626, 67.40899301610995, 69.15934244538272, 66.33702251646812, 79.3840300721583, 59.40747674236163, 68.36115821245029, 89.79871922764347, 83.7453550524189, 75.19069400405357, 60.44231644509973, 62.79532262524339, 60.254290386668956, 62.95938355631186, 31.427097539216035, 88.84528047520377, 10.458310459246706, 8.605970824013694, 7.675120072823757, 7.6741030534702155, 7.670951566905902, 18.686861730607163, 5.799538502677149, 14.862504443011712, 6.599422675539967, 6.532565976820918, 5.648741734644445, 9.653361923256945, 20.854109070140403, 31.203304021350235, 27.89408089488938, 9.537480436306883, 14.271475711744346, 4.752608546597161, 10.274748214874592, 24.39797272327816, 14.106264925403284, 13.316445972048589, 21.028062463744714, 15.54443432313821, 13.846751913646566, 15.346280568553377, 17.602027512044497, 17.652711370728507, 23.542409765211957, 27.494518526655682, 51.70468203306117, 20.689870978293158, 173.8301584231535, 14.997571297330552, 31.659722652701486, 42.62661205864898, 15.553103372319471, 32.09893960504225, 33.59362675114458, 25.74480744309384, 21.64428546427613, 34.613222565247135, 61.68294748727623, 22.605499944005917, 17.424420575709952, 22.74353807097128, 56.40866593882177, 24.67055214867816, 30.408212681711635, 43.9837665551791, 47.44046633663255, 34.05414278205679, 33.16557159112441, 40.59105514515046, 28.06685531808101, 38.625475098035764, 29.81801333240321, 42.401970812744075, 32.533358672590104, 30.485622323973914, 30.803131719446142, 33.37347864733452, 31.96868493958875, 30.56345176357763, 30.976411070702408, 29.74494099647141, 29.124733028610237, 79.96078201152791, 78.96396689699245, 72.17961119401755, 64.39044304500254, 64.31556423604408, 52.704481359335105, 49.8039068601672, 45.913926573432974, 42.874582590984744, 37.15729469204948, 36.182162995857325, 37.13243813130702, 43.667344858435605, 29.375661189553192, 28.39413971499333, 30.24574451218033, 29.260914784343885, 24.501419754129373, 58.382881892361304, 23.539766473257675, 23.539187456731824, 23.538837172560264, 28.24095571686116, 19.64893037615902, 20.56587927535833, 75.62524716064215, 70.97040175216506, 18.668350178457935, 17.70156753960171, 33.545661514997654, 72.97611557715405, 28.741064388668125, 101.47126818107732, 76.31012676050696, 39.06896238508934, 32.194553444746624, 111.24761975589522, 47.67237583235405, 81.63115753477278, 63.016656309574266, 214.8065965544611, 64.52201475953824, 99.53858002601412, 113.61618435031467, 186.50962091815978, 70.60839920774546, 252.6018400588451, 175.87057536765028, 105.84419353259449, 73.54807236695419, 167.89298016845913, 148.42102796866115, 79.12631019229548, 96.65338231712332, 80.76276044256757, 113.81595074878321, 77.9334664206303, 136.75889761371633, 164.49760893370248, 136.21223835160384, 94.97156116096876, 99.91203823833861, 116.6437628876359, 99.57860610970562, 136.10307976301138, 89.66860706372894, 82.4484756689239, 94.42425408567354, 94.87313703526422, 99.85926289091132, 96.0137942868235, 87.45912062711295, 91.95695682850906, 82.58121407406769, 82.50681285756887, 45.182386231513064, 22.199044960419542, 20.29208610944724, 17.41477226361705, 16.460757204165834, 14.548152744730277, 13.58261951353258, 12.633360380532684, 23.436149501793142, 44.99423129787779, 29.66193294443806, 28.683048479200494, 42.934992353675995, 10.721230392017468, 10.712697887682761, 15.153592558502716, 9.74224293904648, 8.805884862149552, 13.212011033706304, 8.803153424696545, 8.802343291540126, 29.807039427505217, 7.842506051333734, 16.40449720773561, 25.8628611074767, 6.8897387639770065, 6.887527460206467, 17.925035970067388, 33.1602750008834, 11.904971552236393, 13.585671718432407, 15.226731351120275, 99.61911968317843, 48.84186584443628, 85.9612789795823, 28.507614336093337, 20.691534533681004, 24.213438199423233, 43.34646457899542, 27.66187361773596, 97.30780418760094, 32.98185363316774, 31.294918129869764, 48.13774726040239, 83.89743006369359, 65.30491732991513, 121.31111446444076, 52.32368974281112, 30.2094007991336, 66.45789246481942, 60.93680232221842, 294.61897339615496, 63.86240701764127, 41.19601221422925, 121.45317388377724, 94.79001945861118, 49.251557173029155, 84.53661563680339, 50.318660093942505, 50.29152103297036, 41.786454662568644, 136.538555521383, 83.47198965097243, 58.54079078319952, 59.14520293715432, 61.24954447991344, 51.61330879486463, 75.22650151975911, 73.705064732809, 82.29075128951158, 76.83537851502484, 68.16087199840261, 66.89399782013726, 56.47495161429296, 61.556654345808646, 52.21687729666607, 54.866940234886975, 52.26052766592869, 62.6162451524346, 47.98991212337741, 39.20883489588905, 35.30821023286347, 35.303277178946736, 32.27032579531771, 27.499122652478473, 26.52743884145361, 26.52473394060393, 26.505382663643523, 25.548105667639653, 25.543629198910274, 26.423662579056927, 94.31090599407366, 22.618802329228345, 21.646220912849255, 20.65251338576087, 19.66536372475233, 18.72587590675002, 18.725614455981002, 18.724055859777653, 18.705547848542643, 18.70474051410762, 17.7332061326736, 17.724153573893947, 15.801037823180474, 15.794342362860634, 15.787713615752875, 15.7769841623354, 15.770507813584453, 36.16557756795984, 38.68949153241466, 77.4226005656001, 17.622892588095883, 78.5061143696827, 30.169294529424885, 130.63422515540282, 81.96724303587705, 44.07731522742582, 41.4761157172871, 120.77939329265006, 118.20444654297337, 51.95523404766515, 29.25286139202557, 50.20220706883414, 78.29173946030221, 82.97425457510802, 213.48146231144136, 66.77455374721711, 63.635862020402776, 173.33598990753993, 122.23757281546192, 82.4201635723619, 177.07408666634169, 47.59142491743624, 436.64643482901835, 132.28259921056434, 135.0454393553149, 86.46082764876286, 112.65759528141157, 89.83397516876347, 102.31299125049863, 57.76184381326726, 160.24903236766232, 145.7458649627963, 119.20538794652911, 107.00059123679776, 156.6615419790804, 88.86205217770402, 107.27308818044897, 129.94529523908335, 126.2405692597816, 131.2928507515622, 126.34673397849096, 116.51119000600521, 107.44635362880028, 102.38354113483595, 98.16664810280055, 94.00588145702184, 91.57153650532806], \"Total\": [792.0, 299.0, 488.0, 334.0, 302.0, 94.0, 485.0, 176.0, 213.0, 110.0, 117.0, 296.0, 200.0, 287.0, 127.0, 281.0, 254.0, 147.0, 210.0, 387.0, 230.0, 139.0, 76.0, 107.0, 232.0, 129.0, 191.0, 238.0, 97.0, 253.0, 20.0510815189163, 19.096459092199517, 20.053577739909777, 20.053586914105196, 14.323467119094927, 13.368852487700895, 14.324700756500429, 12.414311160698352, 12.41432493576038, 12.414480725170941, 39.15909067555302, 110.81433660625554, 10.505199708572587, 9.550542105643792, 9.55074564264145, 9.550930092886802, 33.45540969160963, 8.595961631708256, 8.596210372272422, 31.550965670811674, 74.34173575558624, 7.64144129897411, 7.641427211597222, 29.585810732038166, 8.59810954420413, 16.235569869619393, 34.4044378004655, 51.650749198971596, 14.339411864027248, 6.686805407472199, 81.2199175915448, 34.44443368064089, 49.738829693951544, 30.569180021336493, 86.01759921608365, 127.38155337805931, 52.42328098829519, 42.13242627566581, 37.286405648378604, 36.31250480704717, 59.395114580720964, 36.3804616544857, 101.26629054284984, 120.4051206854693, 42.02805897545589, 95.58603803038538, 52.662521404410235, 42.9783038546686, 200.36295452717482, 334.8394918218566, 254.6559483778039, 485.9388975993249, 488.79099893330465, 131.36749186144425, 82.97713471692471, 67.77740489843184, 85.17981283146153, 191.5720716068828, 176.85628921612997, 83.28760805294883, 302.92097795268137, 119.83160889774929, 129.46029207106616, 143.03689995777708, 206.01087927825364, 116.17977987062571, 161.87204680449273, 329.234456069855, 276.46286927412035, 232.00667287973275, 136.9202062581546, 157.61339974905812, 146.13781357381757, 281.13825964073857, 32.80518250999849, 94.76753986818156, 11.259631548665675, 9.386260943734586, 8.449803065667982, 8.4498083029, 8.449972449377452, 20.667307978843727, 6.577254534327374, 16.92468454779139, 7.518368997187016, 7.521388288274925, 6.58318716873679, 11.290457586013769, 24.4482064592499, 36.69751621566106, 32.908363688102284, 11.277230590508939, 16.944398535394868, 5.645558814844766, 12.24286837725012, 29.192302905118524, 16.956917442139904, 16.01307989580224, 25.442750478868327, 18.843706059972973, 16.950079113655608, 18.83945843420022, 21.62011479992446, 21.68725412940718, 29.236274938016127, 35.84718512939133, 76.39596219443908, 27.30166993871459, 299.73162564692075, 19.789872105474746, 51.05135369988834, 74.79350713600554, 20.785290321185304, 53.15846943923652, 56.86611928043362, 41.68210100394414, 33.045942463573645, 65.65354462578183, 213.2417597825374, 39.743113870255456, 25.49962036250461, 41.7414117914843, 230.68408993483698, 49.4616170730734, 83.70703529527874, 192.67215421676312, 258.50772992205634, 138.57548900337233, 134.20053188044105, 274.4835600776363, 85.50975484985086, 307.54783132123714, 114.30142544222332, 485.9388975993249, 174.29895974765805, 134.86353283420405, 161.87204680449273, 265.51713451371194, 280.2090130655842, 218.4945945557619, 254.6559483778039, 191.5720716068828, 213.51400315766662, 80.72716854132733, 79.75399688608256, 72.94495041137561, 65.16259440276241, 65.1600215447846, 53.488994254471855, 50.57099907891627, 46.67989675372977, 43.75964091159601, 37.92485593209081, 36.95205624033607, 37.924451647961234, 44.729562126537175, 30.14261587058714, 29.169684783283248, 31.111596778827046, 30.14081665512835, 25.27856643986979, 60.28004882176261, 24.305967506857755, 24.30594808718738, 24.305955079852914, 29.166812184331725, 20.414856668215336, 21.38670189324416, 78.73691126944334, 73.90227915386899, 19.441849925212008, 18.469251773014182, 35.00836774967336, 76.79709058841925, 30.13264699650495, 110.88630153517019, 83.54001192028004, 41.78455074069085, 34.03665998105069, 131.00527193047418, 52.50995569112885, 95.14059225644162, 71.79139211409945, 296.3168679341069, 74.81185191758591, 125.43605545952033, 146.60354083563658, 263.8046797995009, 84.51397948335394, 387.31042145043784, 256.0757878518518, 141.25932137264138, 92.1531664211625, 302.92097795268137, 265.9057541324211, 106.61910990333877, 145.10270252905062, 110.41714435257553, 201.71295088755565, 108.55499075071586, 329.234456069855, 485.9388975993249, 334.8394918218566, 165.30555694524622, 189.39612071507693, 276.46286927412035, 202.36983780863133, 488.79099893330465, 159.27882129593667, 124.6512066317598, 215.10057726055638, 232.00667287973275, 327.3281064715791, 281.843679888601, 172.2076026416678, 302.7207773483457, 206.01087927825364, 214.21795438778946, 45.95391266141081, 22.97808526043201, 21.063312977767136, 18.191273710692986, 17.233939057935842, 15.319284637883708, 14.362093160223122, 13.404623577831815, 24.903597167501477, 47.90834253046665, 31.615674891389475, 30.658450300505937, 45.99018258504974, 11.489947031104059, 11.489919788735818, 16.283301204190405, 10.533009129831463, 9.575280494744574, 14.368091382926679, 9.575279649754233, 9.575275148597898, 32.55505480634934, 8.617849491034955, 18.21045621212714, 28.719244011284346, 7.660647485636758, 7.660581313113944, 20.075178822303954, 37.337918422680055, 13.41608371832576, 15.316503068537841, 17.206901223084387, 117.78193044587974, 58.38951553085497, 107.35041785108132, 33.560679580788424, 23.928572562097102, 28.6829245023663, 54.738554566961575, 33.49169027225833, 139.00159379789662, 41.262788646852, 39.375643933139216, 66.2744604844267, 129.60953072072337, 96.49175917243308, 210.51067817545064, 75.02955413748239, 38.413524899543056, 104.93760651666211, 95.75196016951051, 792.9850580094857, 103.91828654962603, 58.64891656409854, 281.13825964073857, 200.93729594069583, 77.7075804964684, 176.85628921612997, 84.8574618890063, 84.99927984318025, 63.23743545731426, 488.79099893330465, 214.21795438778946, 114.03337126125183, 119.59335945186844, 129.46840445983136, 92.61426235089148, 213.2417597825374, 215.10057726055638, 387.31042145043784, 327.3281064715791, 296.3168679341069, 276.46286927412035, 145.3956620235902, 232.00667287973275, 115.04750768633448, 265.51713451371194, 210.47397657397272, 63.385900795312665, 48.75474405914069, 39.97595368333861, 36.07425129479084, 36.07416594510998, 33.143577119428734, 28.270841099527463, 27.29546805653525, 27.295392537458348, 27.295363198188376, 26.320030272277606, 26.31976348213879, 27.295260294881807, 97.4375409535654, 23.393631616543818, 22.418328507586462, 21.442768712883627, 20.466809199455547, 19.49218936033503, 19.49218420604241, 19.49214677180325, 19.492079893692456, 19.491776427422376, 18.51622922357535, 18.51611461586254, 16.5659432647231, 16.56576728959039, 16.5656533818147, 16.56570926188602, 16.56571375091201, 38.0220242848108, 40.893966309929205, 82.77717746901206, 18.51436254698627, 85.68908967000831, 32.16452621198167, 147.07508496597876, 90.55631777943022, 47.661568148605014, 44.79478683038126, 140.1870879049815, 137.33759783535237, 57.43461917970467, 31.19393038180769, 55.56982954802085, 90.53228125886181, 98.27147165506463, 287.3172943076554, 78.88468981778524, 75.01920021521819, 238.3547671737927, 162.3187574611818, 102.85511294744627, 253.70361627666426, 54.51755361229263, 792.9850580094857, 191.60457525415552, 197.51907544683579, 114.97533298326923, 164.0256980542612, 123.41542226413961, 151.3402389921592, 69.90801591863509, 281.843679888601, 248.80000500784888, 188.41704188192526, 162.10109354012053, 307.54783132123714, 130.0715376003249, 185.40498716258548, 274.4835600776363, 265.51713451371194, 327.3281064715791, 302.7207773483457, 280.2090130655842, 229.47458327864567, 204.31414902729887, 213.51400315766662, 249.04840858974015, 233.1695432007498], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.125, -7.1758, -7.132, -7.132, -7.4774, -7.5504, -7.482, -7.629, -7.6291, -7.63, -6.4845, -5.4496, -7.8085, -7.9114, -7.9125, -7.9138, -6.6602, -8.0274, -8.028, -6.7281, -5.8734, -8.157, -8.157, -6.8092, -8.0459, -7.4122, -6.6628, -6.2565, -7.541, -8.3062, -5.8258, -6.6938, -6.3419, -6.8126, -5.824, -5.465, -6.3149, -6.521, -6.6413, -6.6718, -6.2211, -6.6769, -5.7477, -5.6187, -6.5653, -5.8695, -6.3766, -6.5491, -5.2844, -4.9204, -5.1719, -4.7242, -4.7404, -5.7269, -6.0783, -6.2252, -6.0697, -5.5073, -5.5715, -6.0891, -5.2267, -5.8731, -5.8475, -5.8892, -5.7096, -5.9995, -5.8591, -5.5863, -5.6561, -5.7639, -5.9822, -5.944, -5.9853, -5.9414, -6.001, -4.9618, -7.1013, -7.2962, -7.4107, -7.4108, -7.4112, -6.5209, -7.6909, -6.7498, -7.5617, -7.5719, -7.7172, -7.1814, -6.4111, -6.0081, -6.1203, -7.1934, -6.7904, -7.89, -7.119, -6.2542, -6.8021, -6.8597, -6.4028, -6.705, -6.8206, -6.7178, -6.5807, -6.5778, -6.2899, -6.1347, -5.5031, -6.419, -4.2906, -6.7408, -5.9936, -5.6962, -6.7044, -5.9798, -5.9343, -6.2004, -6.3739, -5.9044, -5.3267, -6.3305, -6.5908, -6.3244, -5.416, -6.2431, -6.034, -5.6649, -5.5892, -5.9207, -5.9472, -5.7451, -6.1141, -5.7948, -6.0536, -5.7015, -5.9664, -6.0314, -6.0211, -5.9409, -5.9839, -6.0289, -6.0154, -6.056, -6.0771, -6.2623, -6.2748, -6.3647, -6.4789, -6.48, -6.6791, -6.7357, -6.8171, -6.8856, -7.0287, -7.0553, -7.0293, -6.8672, -7.2637, -7.2977, -7.2345, -7.2676, -7.4451, -6.5768, -7.4851, -7.4852, -7.4852, -7.3031, -7.6658, -7.6202, -6.318, -6.3816, -7.717, -7.7702, -7.1309, -6.3537, -7.2855, -6.0241, -6.309, -6.9785, -7.172, -5.9321, -6.7795, -6.2416, -6.5004, -5.2741, -6.4768, -6.0433, -5.911, -5.4154, -6.3867, -5.112, -5.4741, -5.9819, -6.3459, -5.5205, -5.6438, -6.2728, -6.0727, -6.2523, -5.9093, -6.288, -5.7256, -5.5409, -5.7296, -6.0903, -6.0395, -5.8847, -6.0429, -5.7304, -6.1477, -6.2317, -6.096, -6.0913, -6.0401, -6.0793, -6.1727, -6.1225, -6.2301, -6.231, -6.2139, -6.9246, -7.0144, -7.1673, -7.2236, -7.3472, -7.4158, -7.4883, -6.8703, -6.2181, -6.6348, -6.6683, -6.2649, -7.6524, -7.6532, -7.3064, -7.7482, -7.8492, -7.4435, -7.8495, -7.8496, -6.6299, -7.9651, -7.2271, -6.7718, -8.0946, -8.0949, -7.1384, -6.5233, -7.5477, -7.4156, -7.3016, -5.4233, -6.136, -5.5707, -6.6745, -6.9949, -6.8377, -6.2554, -6.7046, -5.4467, -6.5287, -6.5812, -6.1506, -5.595, -5.8456, -5.2263, -6.0672, -6.6165, -5.8281, -5.9148, -4.3389, -5.8679, -6.3063, -5.2251, -5.473, -6.1277, -5.5874, -6.1063, -6.1068, -6.2921, -5.108, -5.6001, -5.9549, -5.9446, -5.9097, -6.0808, -5.7041, -5.7246, -5.6144, -5.683, -5.8028, -5.8215, -5.9908, -5.9047, -6.0692, -6.0197, -6.0684, -6.3355, -6.6015, -6.8036, -6.9084, -6.9085, -6.9983, -7.1583, -7.1943, -7.1944, -7.1951, -7.2319, -7.2321, -7.1982, -5.9259, -7.3537, -7.3977, -7.4447, -7.4936, -7.5426, -7.5426, -7.5427, -7.5437, -7.5437, -7.5971, -7.5976, -7.7124, -7.7128, -7.7133, -7.7139, -7.7143, -6.8844, -6.8169, -6.1232, -7.6033, -6.1093, -7.0657, -5.6001, -6.0662, -6.6865, -6.7474, -5.6785, -5.7001, -6.5221, -7.0965, -6.5564, -6.112, -6.054, -5.1089, -6.2712, -6.3193, -5.3173, -5.6665, -6.0607, -5.2959, -6.6098, -4.3934, -5.5876, -5.5669, -6.0128, -5.7481, -5.9745, -5.8445, -6.4162, -5.3958, -5.4906, -5.6916, -5.7997, -5.4184, -5.9854, -5.7971, -5.6054, -5.6343, -5.5951, -5.6335, -5.7145, -5.7955, -5.8438, -5.8858, -5.9291, -5.9554], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.7007, 1.6987, 1.6935, 1.6935, 1.6847, 1.6806, 1.68, 1.6762, 1.676, 1.6751, 1.6718, 1.6666, 1.6636, 1.656, 1.6549, 1.6536, 1.6535, 1.6453, 1.6447, 1.6443, 1.6419, 1.6334, 1.6334, 1.6274, 1.6265, 1.6246, 1.623, 1.6229, 1.62, 1.6176, 1.601, 1.5909, 1.5753, 1.5914, 1.5454, 1.5118, 1.5498, 1.5621, 1.564, 1.56, 1.5187, 1.553, 1.4585, 1.4144, 1.5203, 1.3945, 1.4834, 1.5141, 1.2395, 1.0899, 1.1121, 0.9137, 0.8917, 1.2191, 1.3271, 1.3826, 1.3095, 1.0614, 1.0771, 1.3126, 0.8838, 1.1647, 1.1131, 0.9717, 0.7864, 1.0693, 0.8781, 0.4409, 0.5458, 0.6133, 0.9223, 0.8198, 0.8541, 0.2437, 2.3324, 2.3108, 2.3015, 2.2885, 2.2791, 2.279, 2.2786, 2.2746, 2.2495, 2.2454, 2.2449, 2.2343, 2.2222, 2.2186, 2.2163, 2.2131, 2.21, 2.2077, 2.2036, 2.2031, 2.2, 2.1959, 2.1912, 2.1909, 2.1847, 2.1828, 2.1731, 2.1702, 2.1697, 2.1695, 2.1587, 2.11, 1.9849, 2.098, 1.8305, 2.098, 1.8975, 1.813, 2.0853, 1.8708, 1.8489, 1.8935, 1.9521, 1.7351, 1.1349, 1.8111, 1.9945, 1.7681, 0.9669, 1.6797, 1.3627, 0.8981, 0.6798, 0.9718, 0.9775, 0.464, 1.2613, 0.3006, 1.0316, -0.0636, 0.6968, 0.8883, 0.7161, 0.3014, 0.2045, 0.4083, 0.2686, 0.5127, 0.3832, 1.1706, 1.1702, 1.1696, 1.1682, 1.1671, 1.1654, 1.1648, 1.1636, 1.1597, 1.1597, 1.1591, 1.159, 1.1561, 1.1544, 1.1532, 1.1519, 1.1505, 1.1489, 1.1482, 1.1481, 1.1481, 1.1481, 1.1479, 1.1419, 1.141, 1.1398, 1.1397, 1.1395, 1.1377, 1.1375, 1.1291, 1.1328, 1.0914, 1.0896, 1.1129, 1.1245, 1.0167, 1.0835, 1.027, 1.0498, 0.8584, 1.0322, 0.9489, 0.9252, 0.8334, 1.0004, 0.7527, 0.8044, 0.8915, 0.9546, 0.59, 0.597, 0.8819, 0.7738, 0.8674, 0.6079, 0.8487, 0.3016, 0.0969, 0.2807, 0.6259, 0.5406, 0.3172, 0.471, -0.0984, 0.6056, 0.7668, 0.3568, 0.2859, -0.0071, 0.1033, 0.5026, -0.0114, 0.266, 0.226, 1.7824, 1.7649, 1.762, 1.7557, 1.7534, 1.7477, 1.7435, 1.7401, 1.7386, 1.7366, 1.7356, 1.7327, 1.7306, 1.7301, 1.7293, 1.7274, 1.7213, 1.7156, 1.7155, 1.7153, 1.7152, 1.7112, 1.7051, 1.6949, 1.6946, 1.6933, 1.693, 1.6861, 1.6807, 1.6798, 1.6794, 1.6771, 1.6319, 1.6208, 1.5771, 1.6362, 1.654, 1.6299, 1.566, 1.6081, 1.4427, 1.5753, 1.5697, 1.4796, 1.3644, 1.409, 1.2482, 1.4389, 1.5591, 1.3425, 1.3474, 0.8092, 1.3125, 1.4461, 0.96, 1.048, 1.3433, 1.0612, 1.2767, 1.2745, 1.385, 0.524, 0.8569, 1.1326, 1.0952, 1.0509, 1.2147, 0.7574, 0.7283, 0.2504, 0.35, 0.3298, 0.3804, 0.8537, 0.4725, 1.0094, 0.2226, 0.4062, 1.3393, 1.3357, 1.3321, 1.33, 1.3299, 1.3248, 1.3238, 1.3229, 1.3228, 1.3221, 1.3217, 1.3215, 1.319, 1.3189, 1.3178, 1.3164, 1.3139, 1.3115, 1.3114, 1.3114, 1.3113, 1.3103, 1.3103, 1.3083, 1.3078, 1.3042, 1.3038, 1.3034, 1.3027, 1.3023, 1.3014, 1.2961, 1.2846, 1.3021, 1.2639, 1.2874, 1.2329, 1.2518, 1.2733, 1.2745, 1.2025, 1.2015, 1.2512, 1.2872, 1.2499, 1.2062, 1.1823, 1.0544, 1.1848, 1.1869, 1.033, 1.0679, 1.13, 0.9919, 1.2156, 0.7548, 0.981, 0.9713, 1.0665, 0.9758, 1.0339, 0.96, 1.1606, 0.7869, 0.8167, 0.8937, 0.9361, 0.6769, 0.9705, 0.8043, 0.6037, 0.608, 0.4379, 0.4777, 0.4739, 0.5927, 0.6605, 0.5744, 0.3772, 0.4168]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 4, 5, 1, 3, 4, 5, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 2, 3, 5, 4, 4, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 5, 4, 5, 3, 5, 1, 2, 3, 4, 5, 4, 3, 4, 5, 3, 4, 3, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 3, 4, 3, 3, 4, 1, 3, 4, 2, 4, 5, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 2, 5, 3, 4, 5, 5, 2, 1, 2, 4, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 3, 4, 5, 2, 4, 5, 1, 2, 3, 4, 5, 4, 5, 3, 4, 3, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 3, 4, 5, 2, 4, 2, 1, 2, 5, 4, 1, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 5, 3, 4, 3, 3, 1, 2, 3, 4, 5, 2, 4, 5, 5, 3, 4, 1, 2, 3, 4, 5, 3, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 3, 4, 1, 1, 2, 3, 4, 2, 3, 5, 1, 3, 5, 2, 4, 5, 5, 1, 2, 3, 4, 5, 1, 3, 5, 3, 1, 2, 4, 5, 2, 2, 3, 5, 1, 2, 3, 4, 4, 2, 3, 5, 1, 3, 3, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 5, 1, 3, 5, 5, 1, 3, 4, 1, 2, 3, 2, 3, 4, 5, 3, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 5, 3, 1, 2, 3, 4, 1, 2, 3, 4, 2, 1, 3, 4, 4, 1, 1, 2, 3, 4, 5, 2, 4, 5, 3, 4, 5, 3, 3, 4, 3, 1, 2, 3, 4, 5, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 4, 5, 1, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 1, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 1, 3, 3, 2, 5, 4, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 5, 2, 3, 5, 2, 5, 5, 2, 3, 1, 2, 2, 3, 4, 5, 2, 1, 4, 4, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 5, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 2, 5, 5, 3, 4, 1, 3, 4, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 1, 3, 4, 5, 5, 2, 1, 3, 1, 3, 1, 2, 4, 1, 3, 4, 5, 1, 1, 2, 3, 4, 5, 2, 3, 5, 2, 3, 4, 5, 4, 2, 5, 3, 3, 1, 2, 3, 4, 5, 1, 2, 4, 5, 2, 4, 5, 3, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 3, 4, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 5, 5, 3, 4, 5, 3, 3, 4, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 4, 1, 5, 1, 2, 3, 5, 2, 3, 4, 1, 3, 5, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 3, 5, 2, 3, 4, 5, 2, 3, 1, 3, 4, 5, 2, 3, 4, 1, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 5, 1, 2, 3, 5, 1, 3, 4, 5, 1, 3, 4, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5], \"Freq\": [0.22408903036003192, 0.06758240598159693, 0.27744356139813475, 0.4303932170406962, 0.19945574439005476, 0.1857001758114303, 0.13067790149693242, 0.38515592020148504, 0.1031667643396835, 0.23720126996808893, 0.09488050798723557, 0.664163555910649, 0.015813417997872597, 0.35529904142961677, 0.007723892204991668, 0.10813449086988336, 0.4711574245044918, 0.061791137639933345, 0.3302983329600081, 0.0347682455747377, 0.08692061393684423, 0.45198719247159, 0.09561267533052867, 0.9620930089315144, 0.9283058387504103, 0.9488963972162586, 0.03162987990720862, 0.5937541997244498, 0.17508136658541468, 0.1979180665748166, 0.030448933319202554, 0.161741578084295, 0.5054424315134219, 0.32348315616859, 0.9065922733283517, 0.06973786717910398, 0.9012434175639413, 0.08070836575199475, 0.97459281086309, 0.0454881998648407, 0.037906833220700584, 0.7088577812271009, 0.06823229979726105, 0.1440459662386622, 0.6483557549842267, 0.09605270444210767, 0.2281251730500057, 0.024013176110526918, 0.9160592394803235, 0.9306426499058779, 0.41057135407115297, 0.0410571354071153, 0.3421427950592942, 0.15054282982608944, 0.054742847209487064, 0.05486945098701984, 0.06096605665224426, 0.15241514163061065, 0.04267623965657098, 0.6889164401703601, 0.8862794433564463, 0.05908529622376308, 0.8118363290879411, 0.12489789678276017, 0.062448948391380085, 0.9574339963775719, 0.9137608815866503, 0.1253731283892799, 0.8776118987249593, 0.7225606311816879, 0.07474765150155392, 0.12457941916925654, 0.07474765150155392, 0.04051357527504756, 0.048616290330057065, 0.15395158604518072, 0.024308145165028532, 0.729244354950856, 0.9702234017899563, 0.00696094540643428, 0.0696094540643428, 0.09397276298686279, 0.0870118175804285, 0.7413406857852508, 0.9939118827614547, 0.9573586344847597, 0.9755864815366401, 0.9821585617727779, 0.9813398885896021, 0.0721628339320285, 0.24535363536889693, 0.3175164693009254, 0.04329770035921711, 0.3175164693009254, 0.9747882738133209, 0.15116462243102063, 0.5614685976009338, 0.2807342988004669, 0.20095525150456284, 0.7855523467905638, 0.03109015172210085, 0.9327045516630256, 0.2237534868622417, 0.1893298734988199, 0.2926007135890853, 0.26965163801347075, 0.028686344469518166, 0.830714086366651, 0.023734688181904315, 0.11867344090952156, 0.944863615617588, 0.025536854476151026, 0.025536854476151026, 0.9772732570762713, 0.06959866647203264, 0.9047826641364243, 0.06143439204439405, 0.030717196022197024, 0.9215158806659107, 0.02416124904414272, 0.03624187356621408, 0.9302080881994947, 0.8589587148244455, 0.040902795944021214, 0.08180559188804243, 0.11978511951098054, 0.5510115497505105, 0.16769916731537277, 0.11978511951098054, 0.04791404780439222, 0.05700423419850736, 0.04750352849875613, 0.20901552539452697, 0.5747926948349492, 0.1092581155471391, 0.0061607174404298155, 0.024642869761719262, 0.09857147904687705, 0.1170536313681665, 0.7516075277324376, 0.9621504397780063, 0.9193263108794585, 0.04838559530944518, 0.22657738267842562, 0.6930602293693019, 0.06664040667012518, 0.97474940596788, 0.9114126404446898, 0.07325559222162963, 0.7691837183271112, 0.18313898055407407, 0.9401627544481578, 0.02938008607650493, 0.03734514234748945, 0.03734514234748945, 0.38745585185520304, 0.38745585185520304, 0.1493805693899578, 0.8741698108715057, 0.024624501714690303, 0.024624501714690303, 0.04924900342938061, 0.03693675257203546, 0.9654962674877248, 0.7972189466071911, 0.04783313679643147, 0.1594437893214382, 0.8259548469435216, 0.05899677478168011, 0.05899677478168011, 0.00539359817286403, 0.07551037442009642, 0.3020414976803857, 0.03775518721004821, 0.5771150044964513, 0.9211891257124105, 0.06141260838082736, 0.7335817320302818, 0.2626403731960268, 0.9599006711257443, 0.9607281509163436, 0.013531382407272445, 0.013531382407272445, 0.9874118359299449, 0.05035913480371852, 0.07194162114816932, 0.05035913480371852, 0.6978337251372424, 0.12949491806670477, 0.9160575506795785, 0.9826406045440216, 0.8786160991038159, 0.05491350619398849, 0.09962551356095305, 0.8966296220485774, 0.912234727831404, 0.0544996012331341, 0.8447438191135784, 0.08174940184970114, 0.9573609043627865, 0.14929076315212358, 0.836028273651892, 0.01302132661977167, 0.9505568432433319, 0.02604265323954334, 0.01302132661977167, 0.5078336356438324, 0.20657639416020304, 0.043036748783375635, 0.2237910936735533, 0.008607349756675127, 0.1516167488031137, 0.06202503360127379, 0.6684920288137286, 0.03445835200070766, 0.08959171520183992, 0.057737672542692554, 0.30793425356102694, 0.6158685071220539, 0.019245890847564184, 0.05589172469103308, 0.02794586234551654, 0.08383758703654962, 0.8011147205714741, 0.02794586234551654, 0.9891779340761276, 0.8168020509460142, 0.08168020509460143, 0.9624113010504413, 0.03318659658794625, 0.9756134622173153, 0.9819185821556564, 0.018811677810683828, 0.6019736899418825, 0.2821751671602574, 0.07524671124273531, 0.037623355621367656, 0.06850544937106502, 0.8391917547955465, 0.08563181171383127, 0.974751535704873, 0.07453740010835244, 0.8944488013002293, 0.12222484470374989, 0.09249447707310803, 0.30391042466878354, 0.06606748362364859, 0.41622514682898615, 0.9874123407680236, 0.3997122078472042, 0.3172319109898446, 0.28550871989086013, 0.24066260146443663, 0.5749162146094875, 0.1470715897838224, 0.0401104335774061, 0.15159269266677416, 0.7579634633338708, 0.10106179511118277, 0.8444107000190916, 0.10052508333560614, 0.04021003333424245, 0.9666131242742548, 0.6039040514527301, 0.014972827721968514, 0.2146105306815487, 0.16470110494165366, 0.10013157873264773, 0.07152255623760552, 0.829661652356224, 0.9266064975965478, 0.029890532180533797, 0.029890532180533797, 0.10459184521970569, 0.8367347617576455, 0.03486394840656856, 0.9722181875999832, 0.031532282161054984, 0.031532282161054984, 0.8618823790688362, 0.010510760720351661, 0.06306456432210997, 0.019044007690315806, 0.9141123691351586, 0.05713202307094741, 0.9874126248402275, 0.060521802402960324, 0.6657398264325636, 0.21182630841036112, 0.060521802402960324, 0.9467486489366479, 0.829980591023398, 0.046110032834633226, 0.13833009850389966, 0.17153049890786234, 0.16200102674631445, 0.028588416484643726, 0.6289451626621619, 0.9399203291395256, 0.05087946599141333, 0.10175893198282666, 0.8445991354574613, 0.9306695798280544, 0.9908580398400186, 0.9642706612994052, 0.1269820503377702, 0.7872887120941753, 0.07618923020266212, 0.93451400217277, 0.3420051478672022, 0.008769362765825697, 0.12277107872155978, 0.5173924031837162, 0.008769362765825697, 0.9391401330434027, 0.0316564089789911, 0.010552136326330367, 0.021104272652660735, 0.9191477783143857, 0.06338950195271625, 0.8905969557730228, 0.058082410159110184, 0.03872160677274012, 0.9721261924237393, 0.929482620700773, 0.03609641245439895, 0.03609641245439895, 0.09116223548619108, 0.8508475312044501, 0.060774823657460716, 0.8881285286093622, 0.011042851835425988, 0.0883428146834079, 0.905513850504931, 0.032057518490302216, 0.9296680362187641, 0.0886742531310491, 0.8867425313104911, 0.174183691664338, 0.024883384523476857, 0.308553968091113, 0.4727843059460603, 0.019906707618781487, 0.03617363271241123, 0.0924437280428287, 0.15675240842044866, 0.12861736075523994, 0.5868167084457822, 0.010614751086333973, 0.03715162880216891, 0.21760239726984645, 0.10084013532017275, 0.6315776896368714, 0.068282061719581, 0.12680954319350757, 0.23410992589570628, 0.06503053497102952, 0.5104896995225817, 0.04079548892586336, 0.06799248154310561, 0.8907015082146835, 0.303838270291233, 0.007234244530743643, 0.4232033050485031, 0.24234719177991204, 0.02531985585760275, 0.10377040485577897, 0.06423882205357746, 0.4941447850275189, 0.05929737420330227, 0.28166252746568576, 0.9831735566757646, 0.9702211062951163, 0.6666765919777103, 0.0784325402326718, 0.1960813505816795, 0.1038086405256123, 0.2224470868405978, 0.1112235434202989, 0.029659611578746373, 0.5338730084174347, 0.16793779179551505, 0.6237689409547702, 0.07197333934093503, 0.14394667868187005, 0.13875967023128305, 0.8325580213876983, 0.9658369429570445, 0.1327263956298879, 0.14187994015608704, 0.27460633578597493, 0.2059547518394812, 0.24714570220737744, 0.9658472028672099, 0.03012988225657, 0.12428576430835127, 0.16194811712906376, 0.20714294051391877, 0.47454564554097756, 0.9658538441691858, 0.045266305875171116, 0.015088768625057038, 0.015088768625057038, 0.7242608940027379, 0.1961539921257415, 0.021674663395348966, 0.24275623002790842, 0.3294548836093043, 0.16906237448372194, 0.23408636466976884, 0.040154841618823364, 0.9235613572329374, 0.6934464379453902, 0.14754179530752984, 0.13278761577677684, 0.9599952104138919, 0.831402208417159, 0.026819426077972868, 0.0804582782339186, 0.053638852155945736, 0.1347265311721459, 0.1243629518512116, 0.0621814759256058, 0.6736326558607295, 0.9467670405320765, 0.8261616806499644, 0.027538722688332146, 0.11015489075332859, 0.9137687747034141, 0.8972894580265899, 0.5591179206912777, 0.01669008718481426, 0.12517565388610696, 0.20862608981017827, 0.08345043592407131, 0.9588482627906889, 0.9459056056568336, 0.03261743467782185, 0.9821973425225571, 0.9392936099048484, 0.04174638266243771, 0.9796786881751053, 0.8472941459860396, 0.14503233129490767, 0.9870457049316432, 0.011045783736970776, 0.011045783736970776, 0.05522891868485388, 0.05522891868485388, 0.8615711314837206, 0.03131449231857459, 0.22963961033621366, 0.05219082053095765, 0.6889188310086409, 0.9887089618691293, 0.024440308796686287, 0.03360542459544365, 0.3055038599585786, 0.23523797216810552, 0.400210056545738, 0.03701393907324247, 0.05552090860986371, 0.17890070552067194, 0.06168989845540412, 0.6600819134728241, 0.23844912946029698, 0.24590066475593125, 0.29060987652973697, 0.06706381766070853, 0.15648224120831988, 0.2508749703764081, 0.3583928148234401, 0.11946427160781337, 0.23892854321562673, 0.023892854321562674, 0.8623041945818807, 0.061593156755848616, 0.9519095569253836, 0.04614969581027729, 0.02097713445921695, 0.1971850639166393, 0.01258628067553017, 0.7258088522889065, 0.5329819583762643, 0.007724376208351656, 0.19310940520879138, 0.247180038667253, 0.015448752416703312, 0.8787996460696909, 0.06759997277459161, 0.033799986387295804, 0.061504616210364675, 0.015376154052591169, 0.10763307836813818, 0.13069730944702493, 0.6842388553403069, 0.011670097136648726, 0.023340194273297452, 0.03501029140994618, 0.023340194273297452, 0.9219376737952494, 0.9658503243857594, 0.8221345221720032, 0.03425560509050014, 0.1027668152715004, 0.02979677445424684, 0.11918709781698736, 0.8641064591731583, 0.003548066078314231, 0.04257679293977077, 0.34061434351816616, 0.046124859018085, 0.5676905725302769, 0.07070680487530973, 0.24747381706358404, 0.058922337396091436, 0.5892233739609144, 0.03535340243765486, 0.1357674555903189, 0.08354920344019623, 0.11488015473026982, 0.6370626762314963, 0.031330951290073586, 0.13711683418132967, 0.6268198134003642, 0.0979405958438069, 0.13711683418132967, 0.43832671361004444, 0.0864306195850792, 0.3374909907607854, 0.08231487579531352, 0.05350466926695378, 0.9666252544617319, 0.9724095625978196, 0.7504965333734758, 0.019749908772986203, 0.029624863159479306, 0.18762413334336894, 0.035395894241980856, 0.07787096733235788, 0.750392957929994, 0.10618768272594256, 0.02831671539358468, 0.036331145143707416, 0.2283671980461609, 0.37369177862099057, 0.06228196310349843, 0.30102948833357573, 0.14659415780881363, 0.19288704974843898, 0.648100487154755, 0.00771548198993756, 0.9304370872307265, 0.8709680141108305, 0.11612906854811073, 0.9756238625005688, 0.02052596956396011, 0.9647205695061251, 0.979241970788445, 0.005163816642243204, 0.036146716495702426, 0.6532228052437653, 0.21171648233197135, 0.09553060788149927, 0.5805193216579999, 0.21018803025548272, 0.01668158970281609, 0.19350644055266664, 0.1427505830822002, 0.11420046646576017, 0.2819324015873454, 0.042825174924660064, 0.4175454555154356, 0.2733614247863059, 0.030373491642922877, 0.4161168355080434, 0.10630722075023007, 0.17920360069324498, 0.023389144355656963, 0.3274480209791975, 0.3625317375126829, 0.28066973226788355, 0.1255226756890302, 0.023244939942412998, 0.43700487091736434, 0.3440251111477124, 0.0743838078157216, 0.9905459674057522, 0.020251208603275585, 0.02531401075409448, 0.2582029096917637, 0.015188406452456688, 0.683478290360551, 0.9550476374207096, 0.052861023963458084, 0.026430511981729042, 0.08589916394061939, 0.15858307189037424, 0.6739780555340905, 0.038101570808813996, 0.9652397938232878, 0.4382114345261335, 0.13876695426660893, 0.2264092411718356, 0.14607047817537783, 0.04382114345261334, 0.9425831204148524, 0.10064636638835892, 0.5787166067330638, 0.25161591597089733, 0.0754847747912692, 0.003941607197705445, 0.035474464779349, 0.18919714548986133, 0.07489053675640345, 0.6976644739938637, 0.9423348015695735, 0.9467676273432268, 0.04811094695082296, 0.7697751512131673, 0.14433284085246886, 0.04811094695082296, 0.8256217586581143, 0.058972982761293875, 0.058972982761293875, 0.8857036948075211, 0.0885703694807521, 0.9721201753692877, 0.8262317467778887, 0.11803310668255552, 0.030482988463643397, 0.9449726423729452, 0.07369536807728247, 0.7185298387535042, 0.07369536807728247, 0.13817881514490465, 0.8856519193197854, 0.8089833513333503, 0.19034902384314126, 0.9791579929852511, 0.9108428958464764, 0.00901824649352947, 0.07214597194823576, 0.939920770978353, 0.03384657857203796, 0.03384657857203796, 0.556588180962402, 0.07521461904897325, 0.29709774524344434, 0.25792321380244027, 0.29074980465002354, 0.09847977254274992, 0.351713473366964, 0.7785422897842192, 0.03797767267240094, 0.03797767267240094, 0.15191069068960375, 0.9878508223542438, 0.965850586114801, 0.25085004834297514, 0.10034001933719006, 0.1170633558933884, 0.4933384284078511, 0.033446673112396685, 0.6456928956726412, 0.023479741660823318, 0.12913857913452825, 0.18783793328658654, 0.8253824608090903, 0.1572157068207791, 0.98451957704413, 0.043487542070558126, 0.9349821545169996, 0.011970311914179193, 0.9097437054776187, 0.07182187148507517, 0.13089697037323142, 0.6806642459408034, 0.013089697037323142, 0.17016606148520086, 0.026179394074646284, 0.9889801330098202, 0.010124297077367603, 0.020248594154735206, 0.7255746238780115, 0.22948406708699898, 0.016873828462279337, 0.1529424722654639, 0.1529424722654639, 0.08235363891217287, 0.588240277944092, 0.011764805558881838, 0.32326656414265453, 0.4094709812473624, 0.26723369302459443, 0.7114009681872556, 0.01046177894393023, 0.2615444735982558, 0.02092355788786046, 0.1784714359767019, 0.024787699441208594, 0.565159547259556, 0.22308929497087734, 0.009915079776483439, 0.9474614233045695, 0.7910968314378158, 0.06980266159745434, 0.09307021546327246, 0.04653510773163623, 0.9075233207996132, 0.059161810040962645, 0.8400977025816696, 0.023664724016385058, 0.0828265340573477, 0.9747700560154943, 0.9310529986781757, 0.9475798092025758, 0.9854349130779634, 0.8246184527540488, 0.16492369055080977, 0.9423166029351304, 0.05811621668743137, 0.8717432503114705, 0.8254124812486554, 0.034876583714731915, 0.12788080695401702, 0.01162552790491064, 0.947461856753222, 0.10772127195337138, 0.13582247333251174, 0.24822727884907317, 0.04683533563190059, 0.45898628919262585, 0.8208980128584247, 0.06840816773820205, 0.10261225160730308, 0.10694655505964434, 0.05833448457798782, 0.03888965638532521, 0.7972379558991669, 0.9493963098995253, 0.04890697040346494, 0.9536859228675664, 0.9836894865084239, 0.9742353650323571, 0.006305289046114902, 0.0012610578092229804, 0.06935817950726392, 0.3720120537207792, 0.5510822626304425, 0.11037346688737999, 0.008490266683644615, 0.8490266683644614, 0.03396106673457846, 0.7962012311760375, 0.0530800820784025, 0.106160164156805, 0.026300546033775594, 0.9468196572159214, 0.023227778208626836, 0.058069445521567085, 0.5052041760376337, 0.2090500038776415, 0.2032430593254848, 0.021702998146146287, 0.8030109314074126, 0.151920987023024, 0.03255449721921943, 0.6628332032388404, 0.1325666406477681, 0.012051512786160736, 0.15666966622008957, 0.04820605114464294, 0.1438067760207189, 0.06536671637305404, 0.2788979898583639, 0.04357781091536936, 0.4662825767944521, 0.013329920835348265, 0.13329920835348263, 0.853114933462289, 0.3834748935433488, 0.014562337729494258, 0.40289134384934117, 0.16503982760093494, 0.0339787880354866, 0.7928934553046252, 0.13345731425919435, 0.031401721002163374, 0.031401721002163374, 0.08034727501514169, 0.026782425005047232, 0.8838200251665587, 0.9621757303398292, 0.016589236729997056, 0.9747548801166319, 0.17497594559841714, 0.26246391839762573, 0.14872955375865457, 0.02624639183976257, 0.39369587759643854, 0.10551098045588751, 0.5978955559166959, 0.07034065363725835, 0.10551098045588751, 0.1230961438652021, 0.753197214859209, 0.13948096571466834, 0.083688579428801, 0.8202462568033624, 0.0953774717213212, 0.07630197737705696, 0.27081732824089494, 0.09502362394417366, 0.12828189232463444, 0.2470614222548515, 0.26131496584647756, 0.9525463292568533, 0.22613546162795997, 0.017395035509843077, 0.7479865269232523, 0.9909922699573557, 0.9333593232108349, 0.047864580677478716, 0.9306792485254729, 0.939920246194321, 0.427585615234535, 0.008183456750900191, 0.2782375295306065, 0.28028339371833155, 0.0061375925631751435, 0.522638470891912, 0.0029865055479537826, 0.40616475452171447, 0.056743605411121875, 0.008959516643861349, 0.8081472750551827, 0.18520041720014604, 0.08809962543316067, 0.06852193089245831, 0.24472118175877966, 0.09299404906833628, 0.4992312107879105, 0.10295005904042501, 0.051475029520212504, 0.025737514760106252, 0.6305691116226032, 0.18016260332074377, 0.9495182463039176, 0.06528905426553497, 0.9140467597174896, 0.06697207894659081, 0.915285078936741, 0.013366866002750714, 0.02673373200550143, 0.8688462901787964, 0.09356806201925501, 0.027858493074230422, 0.8775425318382583, 0.08357547922269126, 0.07337086378538599, 0.05502814783903948, 0.8804503654246317, 0.9698146258652894, 0.9891789973247918, 0.4614193961102518, 0.006991202971367451, 0.23770090102649336, 0.2726569158833306, 0.027964811885469804, 0.1372391932528223, 0.05575342225895906, 0.3087881848188502, 0.10292939493961673, 0.3945626806018641, 0.5145420635213716, 0.005654308390344742, 0.4806162131793031, 0.9711963791947207, 0.02856459938808002, 0.03482220355187326, 0.03482220355187326, 0.03482220355187326, 0.9053772923487048, 0.8490898737794761, 0.15920435133365177, 0.03410122670917819, 0.10230368012753455, 0.6990751475381528, 0.15345552019130182, 0.03481985805779152, 0.03481985805779152, 0.9053163095025795, 0.9666263270401992, 0.977416981768073, 0.014266649160695853, 0.014266649160695853, 0.06419992122313134, 0.042799947482087555, 0.8631322742220991, 0.14856549459406687, 0.08432095639122715, 0.22485588370993906, 0.16462662914477683, 0.3774366619416834, 0.979351140759281, 0.02511319438111665, 0.14440086769142074, 0.5650468735751246, 0.1192876733103041, 0.14440086769142074, 0.850529846788585, 0.06542537282989115, 0.06542537282989115, 0.9010465504418317, 0.02906601775618812, 0.02906601775618812, 0.05813203551237624, 0.048469821492604884, 0.024234910746302442, 0.7997520546279806, 0.12117455373151222, 0.9423548841988294, 0.2082599818923455, 0.7809749320962956, 0.08873711763549064, 0.06338365545392188, 0.8493409830825532, 0.4258536363901176, 0.5546000846010833, 0.016505954898841768, 0.0033011909797683535, 0.032089540952593094, 0.08022385238148273, 0.6578355895281583, 0.18451486047741028, 0.040111926190741366, 0.01209880682149394, 0.018148210232240912, 0.5746933240209622, 0.03024701705373485, 0.36901360805556516, 0.5340538906172824, 0.1217328721259982, 0.14922094002541714, 0.09031793738380511, 0.10602540475490166, 0.5063368537301707, 0.15659902692685693, 0.12527922154148555, 0.020879870256914256, 0.1879188323122283, 0.977191891764547, 0.9747496637195827, 0.06294379552611942, 0.9231756677164181, 0.060925880282619485, 0.5331014524729205, 0.13708323063589384, 0.07615735035327435, 0.19800911091851334, 0.027284470601460907, 0.013642235300730454, 0.7776074121416359, 0.06821117650365227, 0.10913788240584363, 0.4200848839709159, 0.19150928533968226, 0.03088859440962617, 0.06177718881925234, 0.29653050633241124, 0.0510048761974676, 0.14937142314972654, 0.2623107918726905, 0.06557769796817263, 0.47361670754791346, 0.01583981756687155, 0.005279939188957183, 0.5279939188957183, 0.11615866215705803, 0.33263616890430253, 0.014562654593665651, 0.10193858215565955, 0.021843981890498475, 0.8591966210262735, 0.037516726632086994, 0.037516726632086994, 0.7409553509837182, 0.10317099823823925, 0.08441263492219574, 0.9284006370344196, 0.08997688207193859, 0.8997688207193858, 0.139260826014195, 0.18181274507408793, 0.27078493947204585, 0.1895494576304321, 0.21662795157763667, 0.027335657379875866, 0.05467131475975173, 0.6872965284083075, 0.042956033025519216, 0.18744450774772023, 0.9878408091112765, 0.9891751972919729], \"Term\": [\"000\", \"000\", \"000\", \"000\", \"10\", \"10\", \"10\", \"10\", \"10\", \"1959\", \"1959\", \"1959\", \"1959\", \"1960\", \"1960\", \"1960\", \"1960\", \"1960\", \"1961\", \"1961\", \"1961\", \"1961\", \"1961\", \"academic\", \"acre\", \"acres\", \"acres\", \"act\", \"act\", \"act\", \"act\", \"add\", \"add\", \"add\", \"advantages\", \"advantages\", \"af\", \"af\", \"africa\", \"american\", \"american\", \"american\", \"american\", \"american\", \"amount\", \"amount\", \"amount\", \"amount\", \"ample\", \"appropriated\", \"area\", \"area\", \"area\", \"area\", \"area\", \"around\", \"around\", \"around\", \"around\", \"around\", \"artist\", \"artist\", \"artists\", \"artists\", \"artists\", \"atlanta\", \"attorneys\", \"automobile\", \"automobile\", \"available\", \"available\", \"available\", \"available\", \"away\", \"away\", \"away\", \"away\", \"away\", \"baby\", \"back\", \"back\", \"back\", \"back\", \"back\", \"ball\", \"bankers\", \"baseball\", \"berlin\", \"beside\", \"best\", \"best\", \"best\", \"best\", \"best\", \"bid\", \"bill\", \"bill\", \"bill\", \"billion\", \"billion\", \"birds\", \"birds\", \"board\", \"board\", \"board\", \"board\", \"board\", \"boat\", \"boat\", \"boat\", \"boats\", \"boats\", \"boats\", \"bombs\", \"bond\", \"bond\", \"bonds\", \"bonds\", \"bonds\", \"boy\", \"boy\", \"boy\", \"builder\", \"builder\", \"builder\", \"built\", \"built\", \"built\", \"built\", \"built\", \"business\", \"business\", \"business\", \"business\", \"business\", \"came\", \"came\", \"came\", \"came\", \"came\", \"castro\", \"cattle\", \"cattle\", \"cent\", \"cent\", \"cent\", \"charlie\", \"chef\", \"chemical\", \"chemical\", \"chemical\", \"china\", \"china\", \"city\", \"city\", \"city\", \"city\", \"city\", \"clay\", \"clay\", \"clay\", \"clay\", \"clay\", \"clothes\", \"college\", \"college\", \"college\", \"column\", \"column\", \"column\", \"come\", \"come\", \"come\", \"come\", \"come\", \"commissioner\", \"commissioner\", \"committee\", \"committee\", \"communism\", \"communist\", \"communist\", \"communist\", \"communists\", \"company\", \"company\", \"company\", \"company\", \"company\", \"comprehensive\", \"congo\", \"conspiracy\", \"conspiracy\", \"consumer\", \"consumer\", \"cooled\", \"cooling\", \"cooling\", \"cooling\", \"corp\", \"corporation\", \"corporation\", \"corps\", \"corps\", \"corps\", \"corps\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"country\", \"country\", \"country\", \"country\", \"country\", \"county\", \"county\", \"county\", \"county\", \"court\", \"court\", \"court\", \"court\", \"court\", \"cousin\", \"cream\", \"cream\", \"crisis\", \"crisis\", \"cuba\", \"cuban\", \"daily\", \"daily\", \"daily\", \"daily\", \"daily\", \"dallas\", \"dallas\", \"dallas\", \"dave\", \"davis\", \"davis\", \"day\", \"day\", \"day\", \"day\", \"day\", \"democrats\", \"department\", \"department\", \"department\", \"design\", \"design\", \"design\", \"design\", \"designs\", \"designs\", \"designs\", \"determine\", \"determine\", \"determine\", \"determining\", \"development\", \"development\", \"development\", \"development\", \"door\", \"door\", \"door\", \"drill\", \"drill\", \"drill\", \"driver\", \"driver\", \"driver\", \"drove\", \"east\", \"east\", \"east\", \"east\", \"east\", \"editor\", \"editor\", \"editor\", \"editorial\", \"engineer\", \"engineer\", \"engineer\", \"engineer\", \"entertain\", \"entrance\", \"entrance\", \"entrance\", \"equipment\", \"equipment\", \"equipment\", \"equipment\", \"exempt\", \"eyes\", \"eyes\", \"eyes\", \"facility\", \"faculty\", \"fallout\", \"farm\", \"farm\", \"farm\", \"farmers\", \"federal\", \"federal\", \"federal\", \"federal\", \"federal\", \"feed\", \"feed\", \"feed\", \"feed\", \"fig\", \"fig\", \"file\", \"file\", \"file\", \"fingers\", \"fiscal\", \"fiscal\", \"fiscal\", \"foundation\", \"foundation\", \"foundation\", \"fruit\", \"game\", \"game\", \"game\", \"games\", \"games\", \"gate\", \"gate\", \"general\", \"general\", \"general\", \"general\", \"general\", \"get\", \"get\", \"get\", \"get\", \"get\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"good\", \"got\", \"got\", \"got\", \"government\", \"government\", \"government\", \"government\", \"government\", \"great\", \"great\", \"great\", \"great\", \"great\", \"guy\", \"hair\", \"handling\", \"handling\", \"handling\", \"head\", \"head\", \"head\", \"head\", \"head\", \"heat\", \"heat\", \"heat\", \"heat\", \"heating\", \"heating\", \"hell\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hits\", \"home\", \"home\", \"home\", \"home\", \"home\", \"horses\", \"hospital\", \"hospital\", \"hospital\", \"hospital\", \"hospital\", \"house\", \"house\", \"house\", \"house\", \"house\", \"hughes\", \"hughes\", \"inch\", \"inch\", \"inch\", \"independence\", \"india\", \"india\", \"india\", \"india\", \"industry\", \"industry\", \"industry\", \"industry\", \"installation\", \"interference\", \"interference\", \"interference\", \"inventories\", \"involves\", \"island\", \"island\", \"island\", \"island\", \"island\", \"jane\", \"jones\", \"jones\", \"junior\", \"jury\", \"jury\", \"katanga\", \"kennedy\", \"kennedy\", \"khrushchev\", \"knew\", \"knew\", \"knew\", \"knew\", \"knew\", \"know\", \"know\", \"know\", \"know\", \"laos\", \"last\", \"last\", \"last\", \"last\", \"last\", \"left\", \"left\", \"left\", \"left\", \"left\", \"less\", \"less\", \"less\", \"less\", \"less\", \"level\", \"level\", \"level\", \"level\", \"level\", \"lever\", \"lever\", \"limitations\", \"little\", \"little\", \"little\", \"little\", \"little\", \"local\", \"local\", \"local\", \"local\", \"local\", \"location\", \"location\", \"location\", \"look\", \"look\", \"look\", \"look\", \"look\", \"looked\", \"looked\", \"looked\", \"looked\", \"looked\", \"luck\", \"lumber\", \"lumber\", \"lumber\", \"machinery\", \"machinery\", \"machinery\", \"man\", \"man\", \"man\", \"man\", \"man\", \"management\", \"management\", \"management\", \"management\", \"management\", \"market\", \"market\", \"market\", \"market\", \"market\", \"materials\", \"materials\", \"materials\", \"materials\", \"may\", \"may\", \"may\", \"may\", \"may\", \"measurement\", \"measurements\", \"medical\", \"medical\", \"medical\", \"medical\", \"members\", \"members\", \"members\", \"members\", \"members\", \"men\", \"men\", \"men\", \"men\", \"men\", \"million\", \"million\", \"million\", \"million\", \"minimize\", \"minimum\", \"minimum\", \"moscow\", \"mother\", \"mother\", \"motors\", \"mr\", \"mr\", \"mr\", \"mr\", \"mr\", \"mrs\", \"mrs\", \"mrs\", \"mrs\", \"much\", \"much\", \"much\", \"much\", \"much\", \"must\", \"must\", \"must\", \"must\", \"must\", \"name\", \"name\", \"name\", \"name\", \"national\", \"national\", \"national\", \"national\", \"national\", \"nations\", \"never\", \"never\", \"never\", \"never\", \"never\", \"nice\", \"night\", \"night\", \"night\", \"night\", \"night\", \"nuclear\", \"nuclear\", \"number\", \"number\", \"number\", \"number\", \"number\", \"objectives\", \"oil\", \"oil\", \"oil\", \"oil\", \"old\", \"old\", \"old\", \"old\", \"old\", \"operational\", \"oriental\", \"outdoor\", \"outdoor\", \"outdoor\", \"outdoor\", \"painted\", \"painted\", \"painted\", \"painter\", \"painter\", \"pair\", \"palace\", \"palace\", \"panels\", \"panels\", \"party\", \"party\", \"party\", \"party\", \"patterned\", \"payment\", \"payment\", \"payroll\", \"peace\", \"peace\", \"peace\", \"pension\", \"people\", \"people\", \"people\", \"people\", \"people\", \"per\", \"per\", \"per\", \"per\", \"pieces\", \"pieces\", \"pieces\", \"pieces\", \"pink\", \"pittsburgh\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"planning\", \"planning\", \"planning\", \"planning\", \"plastic\", \"plastic\", \"player\", \"police\", \"police\", \"political\", \"political\", \"political\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"premier\", \"president\", \"president\", \"president\", \"president\", \"president\", \"production\", \"production\", \"production\", \"production\", \"production\", \"program\", \"program\", \"program\", \"property\", \"property\", \"property\", \"property\", \"public\", \"public\", \"public\", \"public\", \"public\", \"puerto\", \"purposes\", \"purposes\", \"purposes\", \"purposes\", \"quantity\", \"question\", \"question\", \"question\", \"question\", \"rain\", \"rated\", \"ratio\", \"rayburn\", \"recreation\", \"recreation\", \"respectively\", \"retail\", \"retail\", \"rhode\", \"rhode\", \"rhode\", \"rhode\", \"rico\", \"right\", \"right\", \"right\", \"right\", \"right\", \"roof\", \"roof\", \"roof\", \"room\", \"room\", \"room\", \"room\", \"ruling\", \"runs\", \"runs\", \"russia\", \"russian\", \"said\", \"said\", \"said\", \"said\", \"said\", \"sales\", \"sales\", \"sales\", \"sales\", \"salt\", \"salt\", \"salt\", \"sat\", \"sat\", \"school\", \"school\", \"school\", \"school\", \"school\", \"schools\", \"schools\", \"schools\", \"schools\", \"section\", \"section\", \"section\", \"section\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seemed\", \"seemed\", \"seemed\", \"service\", \"service\", \"service\", \"service\", \"service\", \"shall\", \"shall\", \"shall\", \"shall\", \"shares\", \"shares\", \"shares\", \"shelter\", \"shelter\", \"shoes\", \"side\", \"side\", \"side\", \"side\", \"side\", \"sign\", \"sign\", \"sign\", \"sign\", \"sign\", \"signs\", \"signs\", \"signs\", \"site\", \"site\", \"site\", \"small\", \"small\", \"small\", \"small\", \"small\", \"smiled\", \"something\", \"something\", \"something\", \"soviet\", \"speaker\", \"speaker\", \"spice\", \"spokesman\", \"state\", \"state\", \"state\", \"state\", \"state\", \"states\", \"states\", \"states\", \"states\", \"states\", \"stations\", \"stations\", \"still\", \"still\", \"still\", \"still\", \"still\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stockholders\", \"stocks\", \"stocks\", \"straight\", \"straight\", \"student\", \"student\", \"student\", \"student\", \"students\", \"students\", \"students\", \"sun\", \"sun\", \"sun\", \"superintendent\", \"susan\", \"system\", \"system\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tax\", \"tax\", \"tax\", \"teachers\", \"teachers\", \"team\", \"team\", \"team\", \"team\", \"temple\", \"temple\", \"texas\", \"texas\", \"texas\", \"texas\", \"textile\", \"textile\", \"textile\", \"thereof\", \"thickness\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"three\", \"three\", \"three\", \"three\", \"three\", \"tiny\", \"today\", \"today\", \"today\", \"today\", \"today\", \"tool\", \"tool\", \"tool\", \"towns\", \"towns\", \"towns\", \"towns\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"transport\", \"trust\", \"trust\", \"turned\", \"turned\", \"turned\", \"united\", \"united\", \"united\", \"united\", \"university\", \"university\", \"university\", \"university\", \"university\", \"us\", \"us\", \"us\", \"us\", \"us\", \"use\", \"use\", \"use\", \"use\", \"use\", \"used\", \"used\", \"used\", \"used\", \"used\", \"vernon\", \"waited\", \"walked\", \"walked\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"war\", \"war\", \"war\", \"war\", \"war\", \"water\", \"water\", \"water\", \"water\", \"water\", \"way\", \"way\", \"way\", \"way\", \"way\", \"week\", \"week\", \"week\", \"week\", \"week\", \"went\", \"went\", \"went\", \"went\", \"west\", \"west\", \"west\", \"west\", \"west\", \"wildlife\", \"woman\", \"woman\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"yankees\", \"yards\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el52473140475086929696339665386\", ldavis_el52473140475086929696339665386_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el52473140475086929696339665386\", ldavis_el52473140475086929696339665386_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el52473140475086929696339665386\", ldavis_el52473140475086929696339665386_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d14c87",
   "metadata": {},
   "source": [
    "Q: What conclusions do you draw from the visualization above? Please address the principal component scatterplot and the salient terms graph.\n",
    "\n",
    "A: The presented visualization displays the 30 most relevant words for each topic, along with their percentage of token occurrence, as illustrated in the salient terms graph. The percentages of the top 30 most relevant words for each of the five topics are: 17.6% for Topic 1, 9.3% for Topic 2, 30.7% for Topic 3, 9.3% for Topic 4, and 25.9% for Topic 5.\n",
    "\n",
    "The red bars in the graph represent the word distribution when a topic circle is clicked. If the length of the bars does not decrease rapidly, it indicates that the respective topic is not highly distinctive. This is observed for Topic 2, 4, and 5.\n",
    "\n",
    "To visualize the results, principal component analysis (PCA) is used to map the topics from their original dimensions into two dimensions. Topics 3 and 4 exhibit a moderate overlap and share similar words related to politics and government. Topic 1 contains words that somewhat relate to Topic 4, which is why the Topic 1 bubble is close to the Topic 4 bubble. Topic 5 is situated far away from the other topics as the words contained in it do not share a common theme other than the common words used in a story. This also holds true for Topic 2.\n",
    "\n",
    "The chart also indicates a weak to moderate overlap between Topics 3 and 4, while Topics 1, 2, and 5 are distinct and distant from all other topics. In other words, Topics 3 and 4 share some similar tokens, whereas Topics 5 and 2 have completely different tokens.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
